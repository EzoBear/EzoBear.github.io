---
layout: post
title: Knowledge Distillation 리뷰
date: 2020-01-02 12:42
category: Model Compression
author: Ezobear
tags: [Knowledge Distillation, Knowledge Transfer, Model Compression]
---

Summary
=============
2012년 알렉스넷의 등장으로 CNN은 이미지 학계의 큰 관심을 끌었습니다. 이와 동시에 알렉스넷, VGG Net, Inception 등의 다양한 모델들이 발표되며 당시 많은 문제들을 State of The Art로 풀어가기 시작했습니다. 그러나 이러한 딥러닝 분야에서는 많은 컴퓨팅 자원이 필요했고, 이를 위해 많은 압축 방법들이 연구되었습니다. 이 방법들 중 정확도를 높이고 모델을 압축하는 방법인 Knowledge Distillation(KD) & Transfer에 대해 조사하였습니다. 과거 KD의 기원이 된 Mimic Learning 부터 최근의 SOTA 모델까지 큰 흐름을 파악하고 정리하였습니다. 자세한 논문 리스트는 레퍼런스에서 확인 가능합니다.
