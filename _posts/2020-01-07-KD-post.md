---
layout: post
title: Knowledge Distillation 리뷰
date: 2020-01-02 12:42
category: Model Compression
use_math : True
author: Ezobear
tags: [Knowledge Distillation, Knowledge Transfer, Model Compression]
summary: 2012년 알렉스넷의 등장으로 CNN은 이미지 학계의 큰 관심을 끌었습니다. 이와 동시에 알렉스넷, VGG Net, Inception 등의 다양한 모델들이 발표되며 당시 많은 문제들을 State of The Art로 풀어가기 시작했습니다. 그러나 이러한 딥러닝 분야에서는 많은 컴퓨팅 자원이 필요했고, 이를 위해 많은 압축 방법들이 연구되었습니다. 이 방법들 중 정확도를 높이고 모델을 압축하는 방법인 Knowledge Distillation(KD) & Transfer에 대해 조사하였습니다. 과거 KD의 기원이 된 Mimic Learning 부터 최근의 SOTA 모델까지 큰 흐름을 파악하고 정리하였습니다.
---

Summary
=============
2012년 알렉스넷의 등장으로 CNN은 이미지 학계의 큰 관심을 끌었습니다. 이와 동시에 알렉스넷, VGG Net, Inception 등의 다양한 모델들이 발표되며 당시 많은 문제들을 State of The Art로 풀어가기 시작했습니다. 그러나 이러한 딥러닝 분야에서는 많은 컴퓨팅 자원이 필요했고, 이를 위해 많은 압축 방법들이 연구되었습니다. 이 방법들 중 정확도를 높이고 모델을 압축하는 방법인 Knowledge Distillation(KD) & Transfer에 대해 조사하였습니다. 과거 KD의 기원이 된 Mimic Learning 부터 최근의 SOTA 모델까지 큰 흐름을 파악하고 정리하였습니다.

Knowledge Distillation 리뷰
=============
2012년 알렉스넷의 등장으로 CNN은 이미지 학계의 큰 관심을 끌었습니다. 이와 동시에 알렉스넷, VGG Net, Inception 등의 다양한 모델들이 발표되며 당시 많은 문제들을 State of The Art로 풀어가기 시작했습니다. 그러나 이러한 딥러닝 분야에서는 많은 컴퓨팅 자원이 필요했고, 이를 위해 많은 압축 방법들이 연구되었습니다. 이 방법들 중 정확도를 높이고 모델을 압축하는 방법인 Knowledge Distillation(KD) & Transfer에 대해 조사하였습니다. 과거 KD의 기원이 된 Mimic Learning 부터 최근의 SOTA 모델까지 큰 흐름을 파악하고 정리하였습니다. 자세한 논문 리스트는 레퍼런스에서 확인 가능합니다.

Introduction
=============
과거부터 지금까지 기계학습 방법을 이용한 알고리즘들은 앙상블 기법을 통해 SOTA를 보여주었습니다. 그러나 이러한 앙상블 방법의 장점때문에 매우 느리고, 무겁다는 큰 단점들이 간과되고있었습니다. 따라서 이러한 문제가 Cristian al.3 에 의해 제시되었고, 이러한 문제를 해결하기 위해 Mimic Learning이라는 학습기법을 제안하였습니다. 이는 기존의 앙상블의 거대하고, 큰 모델에서 예측된 확률을 Pseudo Label로 사용하는 방법으로 KD분야의 기원이 되었습니다[1].

이 Mimic Learning의 배경과 마찬가지로, Deep Learning에서도 모델이 너무 무겁고 너무 느리다는 문제가 제기되었습니다.[2]. 또한 2014년에는 Gradient Vanishing 문제로 네트워크를 깊게 쌓는것 마저 어려운 일이었습니다. 따라서 저자들은 심층 학습 네트워크가 더 깊어야 하는지에 대해 의문을 제기하였고, 또한 심층 네트워크에서 얕은 네트워크로 미믹 학습을 수행함으로써 얕은 네트워크로 깊은 네트워크의 성능을 내기 위해 연구를 진행하였습니다. 
 그러나 본 논문의 한계는 모델을 압축하는 것이 아니라, 얕은 모델로 깊은 모딜의 예측 분포를 모방하여 정확도를 높일 수 있다는 점에서만 기술하고 있다는 점 입니다. 
 
 이러한 문제는 이후 2015년에 이 Mimic Learning 연구에서 영감을 받은 구글의 Knowledge Distillation을 이용한 딥러닝 모델 압축에 관한 연구가 시작되었고, Fitnet, 연구를 지나면서 점차 틀이 잡히고, 구체화됩니다. 그러면서 Attention Transfer, Jacobian Trnasfer 등 다양한 연구들이 등장하게 되는데, 이러한 흐름과 각 모델에 대해서는 아래서 더 자세하게 다루겠습니다.

 본 리뷰에서는 세가지 파트로 나눠 KD 분야에 대해 설명하고있습니다. 첫째 파트에서는 위의 큰 흐름에 대해 조금 더 자세하게 다룹니다. 두번째 파트에서는 이러한 큰 흐름에서 파생되어나온 다양한 KD 방법론에 대해 다룹니다. 세번째에서는 이러한 KD가 적용된 Object Detection 논문에 대해 다루도록합니다.

A large Stream of Knowledge Distillation’s Development
=============
제1장은 KD의 분야의 큰 흐름에 대해 다룹니다. 이 큰 흐름은 4개의 주요 연구로 이어지는데, Mimic Learning과 Deep Learning의 Model Compression을 제안한 연구, 그리고 그것을 바탕으로 한 KD라는 용어를 제안한 연구입니다. 이후 FitNet 연구를 거치면서 점차 Model Compression의 틀을 발전시킬 뿐만 아니라 정량적 평가를 위한 큰 틀또한 구축하게 됩니다[1][2][3][4].

## 1. Model Compression & Do Deep Nets Really Need to Deep?

과거의 기계학습 분야에서는 앙상블과 거대 모델이 좋은 성과를 거두었습니다. 그러나, 이러한 모델들은 많은 컴퓨팅 자원을 필요로 했고, 이러한 문제들을 해결하기 위해 Mimic Learning이 제안되었습니다[1]. Mimic Learning의 핵심 아이디어는 대형 혹은 앙상블 모델의 Prediction을 Soft Label로 사용하여 소규모 모델을 Supervised Learning하는 기법입니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-01-02-KnowledgeDistillation-Review/Figure1.png?raw=true" alt="Figure 1">

표 1은 본 논문에서 제시하는 데이터를 제가 재구성 한 표입니다. 여기서 정확도의 지표로 볼 수 있는 RMSE는 비슷하지만, 추론 시간과 모델 크기는 매우 큰 차이로 성공적인 Model Compression을 하였다고 볼 수 있습니다. 

이후 딥러닝이 Gradient Vanishing 문제로 더 이상 깊게 네트워크를 쌓는것이 어려웠던 시기에 이와 같은 방법을 딥러닝에 적용시킨 Do Deep Nets Really Need to Deep? 논문도 발표되었습니다[2]. 이러한 논문들은 이후 KD연구분야의 기원이됩니다.

## 2.Distilling the Knowledge in a Neural Network

KD라는 용어가 처음 사용된 구글의 논문입니다[3]. 여기서 Distilling은 증류로 원하는 특성 성분을 불순물과 혼합한 혼합물에서 분리하는 방법인데, 구글의 KD 논문에서는 신경망에서 학습된 파라미터의 결과를 지식이라고 하며, 이러한 지식은 많은 불순물이 혼합되어있는 파라미터로부터 순수한 지식을 증류하는 방법을 제안하였고, 이를 통해 Model Compression을 수행하고자 하였습니다.

$$ KD(x)_i = \frac{exp(x_i/T)}{\sum_{j}^{ }exp(x_j/T))} $$

위의 수식은 논문에서 제안하는 KD방법이며, 기본적으로는 Mimic Learning과 같습니다. 다만 지식을 Distilling하는 온도 하이퍼파라미터 T가 추가되었는데, 이 T를 통해 Distilling된 정보(Softmax로 구해진 확률맵)을 학습하게 됩니다. T는 1로 설정하면 일반적인 SoftMax함수가됩니다. 이때 T 값이 커지면 Expoential로 들어가는 입력값이 작아지므로 결과값이 천천히 증가하게 됩니다. 즉, SoftMax함수를 거칠때 한 값이 너무 큰 값을 가지는 것을 방지합니다. 이는 전체 확률 분포를 부드럽게하며, 이를 통해 학습할 경우 좀 더 일반적인 지식을 배움으로써 모델이 일반화된다 하였습니다. Inference Phase에서는 T값을 1로 사용합니다. 이때 모델이 일반적이게 학습된 것을 보여주기 위해 Mnist 데이터셋에서의 KD를 보여주는데, 숫자 3을 제외하고 학습하였는데도 테스트에서 98.6%의 정확도를 보였다고하였습니다. 이 실험을 통해 KD논문에서는 학습하지 않은 데이터까지 어느정도 맞출수 있고, 모델 자체가 일반화된 것을 보여주었습니다. 다만 본 논문의 한계로는 모델의 압축정도에 대한 내용은 자세히 나와있지 않다는 점입니다. 이러한 모델 압축의 정량적인 평가 틀은 이후 Fitnet에서 잘 다져놓게 됩니다.

## 3.FITNETS: HINTS FOR THIN DEEP NETS
Fitnet은 KD분야에서 가장 중요한 논문이며 많은 논문의 Baseline으로 으로 사용되고있습니다[4]. 기존의 Model Compression 혹은 KD논문들은 대부분 정확도 상승 측면에 초점을 맞추어 실험결과를 보여주었습니다. 그러나 Fitnet논문에서는 정확도 측면뿐만 아니라 모델의 파라미터 수, 계산 속도, 정확도 등을 고려하여 압축과 속도 측면에서의 정량적인 측정을 수행했습니다. 따라서 이러한 정량적 평가의 틀을 구축함으로써 이후 많은 논문들에 영향을 미쳤습니다. 


<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-01-02-KnowledgeDistillation-Review/Figure2.png?raw=true" alt="Figure2">

그림 1의 (a)는 Fitnet의 기본 아이디어인 Teacher Student Framework를 볼 수 있습니다. 이는 Fitnet의 기본 아이디어로, Teacher(넓고 얇은 모델,그러나 여전히 깊은)의 행동을 Student(좁고 깊은 모델)이 배우는다는것인데 기존의 Mimic Learning 방법들과의 차이점은, 단순히 이러한 모델의 형태가 정해져있을뿐만 아니라  Hints(Intermediate Hidden Layer)를 기반으로 학습한다는 점입니다. 이를 통해 오른쪽 빨간색 상자(Student)가 오른쪽 초록색 상자(Teacher)를 흉내내게 하는것입니다. 다만 좁은 Student는 넓은 Teacher의 Hints를 흉내낼 수 없습니다. 따라서 이를 해결하기 위해 그림 1의 (b)를 도입하게되는데, 그림 b의 W_r은 흉내내는 것을 도와주는 모듈입니다. 좀 더 기술적으로 얘기하면 좁은 채널을 갖는 Student가 넓은 채널을 갖는 Teacher의 Hints의 Feature Map을 학습하기 위해 사이즈를 매핑시켜주는 Regressor입니다. 즉,(a)와 같은 방법으로  전체적으로 파라미터수를 줄이면서 Multipication 횟수를 줄이므로써 가볍고, 빠른 모델을 만들고, (b)의 방법을 통해 이러한 모델인 Teacher를 따라할 수 있는 Student와 학습 도우미(Regressor)를 만들었습니다. 이 후 (c)와 같이 KD를 수행함으로써 가볍고, 빠르고, 정확한 모델을 만들 수 있다고하였습니다.

$$L_{HT}(W_{Guided}, W_r) = 1/2 ||{u_h(x; W_{Hint})- r(v_g(x; W_{Guided}; W_r)}||^2$$

수식은 그림(c)의 학습방법에 대한 내용입니다. $W_{Guided}$는 연구의 특정 Layer의 Weight 파라미터인 Feature Map을 나타내며, $W_{hint}$는 힌트가 되는 Teacher의 특정 Layer의 Weight 파라미터인 Feautre Map을 가리킵니다. 모델은 Regressor를 사용하여 High Dimension인 Teacher의 Hints과 Low Dimension인 Student의 Hints간의 차이를 학습합니다. 

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-01-02-KnowledgeDistillation-Review/Figure3.png?raw=true" alt="Figure3">

Figure 3의 좌측 표는 Fitnet의 실험 결과로, 기존의 방법과 비교하였을때 파라미터 대비 정확도가 향상된 것을 볼 수 있습니다. 우측 그래프는 추가적으로 기존의 일반적인 Back Propagation 방식과 Knowledge Distillation 방식, Hints Training 방식을 비교한 실험인데, 오퍼레이션 수를 고정 후 Layer 수를 변화하면서 정확도를 측정한 실험입니다. 오퍼레이션 수가 많을때(107M)일때는 KD방식과 Hints방식이 모두 잘되었는데, 오퍼레이션 수가 적을때 KD로는 학습이 안되는 문제 또한 극복하였습니다. 따라서 보다 적은 파라미터로 학습이 가능하게 된것입니다. 아래 Figure4는 Fitnet의 학습 결과로, 모델의 레이어별, 파라미터의 개수, 연산 횟수, 정확도, 속도 증가율, 압축율 등 다양한 관점에서 Fitnet의 성능을 정량적으로 보여주고 있습니다. 이러한 기반은 향후 많은 논문에 영향을 미치고, Baseline으로 사용됩니다. 

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-01-02-KnowledgeDistillation-Review/Figure4.png?raw=true" alt="Figure4">

Various version of Knowledge Distillation
=============
Fitnet 이후로 KD에 대한 다양한 버전의 테크닉들이 발표되었는데, 이러한 연구 흐름은 크게 2가지 방향으로 진행됩니다.

첫번째 연구 흐름은 KD 자체를 강화하는 방향입니다. 이는 주로 어떤 파라미터를 학습하는데 사용할건지에 대해 다룹니다. 두번째 흐름은 KD 후 Transfer를 강화하는 방향압니다. 주로 네트워크가 서로 많이 다를 경우(크기, 깊이 등) Transfer가 잘 되지 않는 문제를 해결하기 위한 방법을 다룹니다. 

이러한 흐름에서 첫번째 케이스의 대표적인 연구로는 Attention을 학습하거나, Gan을 사용하거나, Jacobian을 사용하는 등의 연구가 있습니다[5][6][7]. 두번째 케이스의 대표적인 연구로는 Mutual Learning, Knowledge Projection, Teacher Assistant 등을 사용한 연구가 있습니다[8][9][10]. 각 논문의 자세한 내용들은 각 장에서 Key 아이디어 위주로 다시 보도록 하겠습니다.

업데이트 예정...
## 1. PAYING MORE ATTENTION TO ATTENTION: IMPROVING THE PERFORMANCE OF CONVOLUTIONAL NEURAL NETWORKS VIA ATTENTION TRANSFER

## 2. Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks

## 3. Knowledge Transfer with Jacobian Matching

## 4. Deep Mutual Learning

## 5. Knowledge Projection for Effective Design of Thinner and Faster Deep Neural Networks

## 6. Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher
Reference
=============
[1]Buciluǎ, Cristian, Rich Caruana, and AlexandruNiculescu-Mizil. "Model compression." Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006.<br>
[2]Ba, Jimmy, and Rich Caruana. "Do deep nets really need to be deep?." Advances in neural information processing systems. 2014.<br>
[3]Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." arXiv preprint arXiv:1503.02531 (2015).<br>
[4]Romero, Adriana, et al. "Fitnets: Hints for thin deep nets." arXiv preprint arXiv:1412.6550 (2014).<br>
[5]Zagoruyko, Sergey, and Nikos Komodakis. "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer." arXiv preprint arXiv:1612.03928 (2016).<br>
[6]Xu, Zheng, Yen-Chang Hsu, and Jiawei Huang. "Training shallow and thin networks for acceleration via knowledge distillation with conditional adversarial networks." arXiv preprint arXiv:1709.00513 (2017).<br>
[7]Srinivas, Suraj, and François Fleuret. "Knowledge transfer with jacobian matching." arXiv preprint arXiv:1803.00443 (2018).<br>
[8]Zhang, Ying, et al. "Deep mutual learning." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br>
[9]Zhang, Zhi, Guanghan Ning, and Zhihai He. "Knowledge projection for deep neural networks." arXiv preprint arXiv:1710.09505 (2017).<br>
[10]Mirzadeh, Seyed-Iman, et al. "Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher." arXiv preprint arXiv:1902.03393 (2019).<br>
[11]https://github.com/HobbitLong/RepDistiller<br>
[12]Heo, Byeongho, et al. "Knowledge transfer via distillation of activation boundaries formed by hidden neurons." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.<br>
[13]Xu, Jiaolong, et al. "Training a Binary Weight Object Detector by Knowledge Transfer for Autonomous Driving." 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.<br>
