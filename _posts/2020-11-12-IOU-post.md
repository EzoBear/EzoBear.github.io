---
layout: post
title: Localization & Bounding Box Regression for Object Detection Focusing on IOU
date: 2020-01-02 12:42
category: IOU
author: Ezobear
tags: [IoU, Intersection Over Union, GIoU, DIoU, CIoU]
excerpt: 여태까지의 객체 검출에서의 연구는 검출기의 아키텍처 변경을 통한 Feature Level의 성능향상이나 Focal Loss등을 통한 Classification에서의 성능 향상을 주로 다루었습니다. 이번 포스팅에서 IOU Loss의 발전에 대해 다루려고 합니다. 이러한 연구 방향은 기존의 발전과는 다르게 Boundung Box Regression에서의 성능 향상을 보였다는 점에서 Novelty가 있습니다. 이는 객체검출기에서의 새로운 발전 방향을 제시합니다.
use_math: true
---

Introduction
=============
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure1.png?raw=true" alt="Figure1">

Localization은 객체검출기에서 가장 중요한 능력 중 하나입니다. 객체검출기는 Localization을 위해 Feature Map을 기반으로 Object가 존재하는 위치에 Bounding Box를 그리게 됩니다. 이러한 동작은 Bounding Box Regression이라 합니다. 이러한 Bounding Box Regression의 성능을 측정하기 위해서 실제 객체와 예측된 객체간의 IOU를 계산함으로써 Localization 성능을 측정하게 됩니다. 

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure2.png?raw=true" alt="Figure2">

객체검출 문제에서 Localization의 발전은 크게 2가지 방향으로 발전됩니다. 첫번째는 다양한 형태의 Bounding Box Regression을 수행함으로써 보다 정확한 객체의 Localization을 Estimation하는 방향으로 Segment Detection, Polygonal Detection, Free-Shape Detection 혹은 Instance Segmentation 등 여러 이름으로 불리고 있습니다[1][2][3][4][5][6][7]. 이 경우는 다양한 Shape의 Detection을 수행하기 때문에 다양한 모양의 IOU를 구하는 방법에 대해 다루는 경우도 있습니다. 

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure3.png?raw=true" alt="Figure3">

두번째 방향은 최근 연구가 되고 있는 방향으로, 기존 IOU Measure 자체에 주목하여 Jarccard Index의 Measure 및 Loss로써의 한계 지적하고 이를 개선하는 방향입니다[8][9]. 두번째 방향이 중요한 이유는 다음과 같습니다. 여태까지의 객체 검출에서의 연구는 검출기의 아키텍처 변경을 통한 Feature Level의 성능향상이나 Focal Loss를 통한 Classification에서의 성능 향상을 주로 다루었다면, 이번 포스팅에서 다룰 두번째 방향은 Boundung Box Regression에서의 성능 향상을 보였다는 점입니다. 이는 여태까지 없던 시도였고, 충분히 객체검출기에서의 새로운 기회로 보이기 때문에 중요한 연구분야로 자리잡을것으로 보입니다. 따라서 본 포스팅에서는 두번째 소개해드린 IOU의 발전에 대해 중점적으로 다루고자 합니다.

IOU(Intersection Over Union)
=============

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure4.png?raw=true" alt="Figure4">

IOU는 객체검출 문제에서 전통적으로 Localization을 평가하기 위한 Measure로 사용되어왔습니다. 이러한 IOU는 두개의 객체가 겹치는 부분을 측정함으로써 Localization능력을 평가해왔습니다. 이러한 IOU는 기존의 Jaccard index 혹은 Jaccard similarity coefficient라고 불리는 Measure로 측정되어왔습니다. 

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure5.png?raw=true" alt="Figure5">

Jaccard Index는 위와 같이 표현되며, 이는 A와 B의 합집합에서의 A와 B의 교집합으로 표현됩니다. 이러한 Jaccard Index의 성질은 결과가 0과 1사이에서 정의된다는 것입니다. 이러한 Property는 박스의 Width, Height 정보를 Encoding한 정보이며, 이러한 정보는 객체검출 문제에서 두개의 객체사이의 겹치는 비율을 Nomalization된 결과 혹은 Probability로써도 볼 수 있도록 합니다. 또한 이를 통하여 IOU가 Scale Invariant 하다는 장점 또한 갖게됩니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure6.png?raw=true" alt="Figure6">
 
또한 이러한 성질을 통해 유사성이 아닌 거리로써 표현될 수도 있습니다. 위 수식은 Jaccard Similarity 1에서 빼줌으로써 기존과 반대의 성질을 부여하게됩니다. 기존에는 교집합이 크면 1, 작으면 0에 가까운 수로 표현되었는데, 1에서 빼줌으로써 많이 겹치면 0, 겹치지 않는다면 1로 표현되게 됩니다. 이는 객체검출 문제에서 겹치는 객체끼리의 거리는 가깝고, 겹치지 않는 객체끼리의 거리는 멀다는 특징을 잘 살린 Distance Mesaure로써 사용할 수 있게 해줄뿐만 아니라 Loss로써도 직접적으로 사용할 수 있도록 해줍니다.

GIOU(Generalized IOU)
=============

GIOU는 IOU의 확장버전입니다. 핵심부터 말하자면 GIOU는 기존의 Jarcard Index에 Regularize Term을 추가한 형태로 크게 2가지 관점에서의 개선을 보입니다. 첫번째는 Jarcard Index가 2개의 객체가 Overlapping되어있지 않다면, Gradient 값이 0으로 학습에 관여할 수 없는 Gradient Vanishing 문제가 발생한다는 점과 얼마나 두 객체가 멀리 떨어져있는지는 반영하지 못한다는 점을 개선합니다. 두번째는 GIOU가 Jarcard Index의 IOU에 비해 좀 더 타이트하고 실제와 유사한 Stronger Shape에 대한 정보를 반영함을 보이고, 이를 통해 성능적인 개선을 보입니다. 자세한 내용은 아래에서 다루겠습니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure7.png?raw=true" alt="Figure7">

위 알고리듬 표는 GIOU의 간략화된 버전입니다. Input에서 A,B는 각 객체의 Bounding Box이며, 이들은 Scale 공간 S에 속하고, S는 N차원의 R Space에 속합니다. 이때 C는 Smallest Enclosing Convex Object로, 2개의 박스 A와 B를 포함하는 가장 작은 박스입니다. 이때 Regularize term은 C에서 A와 B의 영역을 뺀 나머지 값이 되는데, 이를 IOU에서 마이너스함으로써 GIOU를 구할 수 있습니다. 이를 시각화해서 표현하면 아래와 같습니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure8.png?raw=true" alt="Figure8">

위의 그림이 C를 설명하는 그림으로, 빨간색이 Predicted Box이고, 초록색이 Ground Truth라 할때 이 두개를 감싸고 있는 검은 Box가 Smallest Enclosing Convex Object인 C를 의미합니다. 본 연구에서는 C를 통하여 첫번째 문제인 Jarcard Index가 2개의 객체가 Overlapping되어있지 않다면, 값이 0으로 학습에 관여할 수 없다는 점과 얼마나 두 객체가 멀리 떨어져있는지는 반영하지 못한다는 점을 개선하였습니다. 자세한 내용은 몇가지 시나리오와 함께 아래서 확인해보겠습니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure9.png?raw=true" alt="Figure9">

위 그림에서는 3개의 시나리오를 다루고있는데, 좌측부터 $A$와$B$가 완전히 겹칠때, $A$와 $B$가 겹치지 않았지만 딱 붙어있을때, $A$와 $B$가 겹치지 않고 거리가 멀때입니다. 첫번째 시나리오에서 보면, $A$와 $B$는 완전히 겹쳐있습니다. 이는 $IOU$가 $1$이고, $C$는 $0$인 상황입니다. 이는 $GIOU$의 $Regularzie term$의 분모가 $0$이되게 됩니다.즉, $GIOU = IOU - 0$임으로 $IOU$가 되게됩니다. 이는 완전히 겹치는 상황에서는 $IOU$와 동일하게 동작함을 보입니다. 이 부분에 대해서 좀 더 자세하게 설명드리면, $GIOU = IOU - (A^c - U) / A^c$입니다. 이때 $A^c$는 $C$의 $Area$를 뜻하며, $U$는 $A^p + A^g -I$ 를 뜻합니다. 이때 $I$는 $B^p$와 $B^g$의 Interecntion입니다. 이때 $p$와 $g$는 $predict$와 $gt$를 의미합니다. 여기서 $A$와 $B$가 완전히 겹치면, $A^p = A^g = I = A^c$가 되게됩니다. 즉, 이를 $GIOU$에 대입하여 생각하면 분모가 다 상쇄되어 $0$이되는 것을 확인할 수 있습니다. 두번째 시나리오의 경우 겹치는 영역도 없지만 딱붙어서 $C$와 $A$와$B$의 합집합이 같은 경우입니다. 위와 같이 계산해보면 이 경우는 $GIOU$가 $0$이 나오는 것을 확인할 수 있습니다. 세번째 시나리오는 멀리 떨어져있는 경우입니다. 이때 $A$와 $B$의 합집합 대비 $C$가 커지게되면 점점 $0$에 가까워지고, 이때 $\lim{\frac{\midAUB\mid/}{\midC\mid\}rightarrow0}$으로가면 $GIOU$는 결국 $C/C$ $Term$만 남음으로, $Regularization Term$은 $1$이되어 $GIOU = 0 - 1$로 $-1$이 될 것입니다. 이는 두개의 Bounding Box의 거리가 멀때 $IOU$와 다르게 얼마나 거리가 먼 지를 평가할 수 있음을 보입니다. 이러한 내용을 $Property$로 정리해보면 아래와 같습니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure11.png?raw=true" alt="Figure11">

위 알고리듬 표2는 위에서 다룬 알고리듬1의 좀 더 자세한 표현입니다. 알고리듬2의 내용은 위에서 정리한 내용과 같습니다. 좀 더 자세한 내용을 보고싶으시면 이 알고리듬 2를 참조하시면 됩니다. 이때 $L_{GIOU} = 1 - GIOU$로 되어있는데, 이는 GIOU가 -1 부터 1의 범위를 갖기 때문에 이를 Loss로써 활용하기 Positive 영역으로 Shift하기 위함입니다. 이를 통해 GIOU는 0부터 2의 양수 범위를 갖게 됩니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure12.png?raw=true" alt="Figure12">

위 그림은 이러한 내용은 시각화한 그림입니다. $L_n Norm$과 IOU, GIOU를 비교한 그림으로, $L_n Norm$은 상자의 크기나 위치 등에 영향을 안 받고 다 다른 케이스를 같은 값으로 표현한 반면에 IOU는 이를 반영하여 표현하고있습니다. GIOU의 경우 위의 케이스에서도 봤듯이 IOU와 같은 경향성을 갖지만, 좀 더 타이트한 값을 갖고있습니다. 이러한 IOU와 GIOU의 경향성은 아래의 그래프에도 확인할 수 있습니다. 본 연구에서는 제안한 GIOU를 바탕으로 Faster R-CNN, Mask R-CNN, Yolo 등을 학습시켜 성능향상을 보았는데요, 얼마나 향상되었는지는 다음 챕터에서 다룰 DIOU와 CIOU를 다루고 이들과 함께 보도록 하겠습니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure13.png?raw=true" alt="Figure13">

## 사용기

GIOU를 Pretrained Model을 Fine tune하는 형태가 아닌, Pretrain Model이 없는 상태로 Loss를 적용해보니 문제가 발생했습니다. 초기 Bounding Box Regression의 결과가 $x_1,y_1,x_2,y_2$에서 $x_2, y_2$값이 $x_1, y_1$ 값보다 작게 나오는 현상을 발견했습니다. 따라서 이러한 문제를 해결하기 위해 Bounding Box Regression을 $x_c,y_c,w,h$로 예측하게 한 후 center based representation을 $x_1,y_1,x_2,y_2$로 바꾸어 GIOU에 적용함으로써 문제를 회피하였습니다. 이러한 방법은 center를 기반으로 표현할 경우 중심 point를 기반으로 w,h를 표현하는 것이기 때문에 $w,h$가 양수의 값을 갖는다면 항상 $x_2, y_2$값이 $x_1, y_1$ 값보다 크게 나오기때문에 가능합니다. 다만 이러한 방법은 매번 box representation을 변경해주어야 하여 over-head가 발생하고, 간접적으로 학습하였기때문에 이 정보들이 잘 반영되는지에 대해서는 생각해봐야 할 것 같습니다.


Distance IOU(DIOU)
=============

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure14.png?raw=true" alt="Figure14">

GIOU는 IOU가 Non-overlapping case에서 Gradient Vanishing이 발생하는것을 개선하고, 좀 더 타이트한 정보를 제공한다는 장점을 갖게하였습니다. 다만 이러한 GIOU에도 여전히 Slow Convergence, Inaccurate Regression 등의 몇가지 문제가 존재합니다. 특히 GIOU의 가장 큰 한계는 위의 그림에서 보듯 Horizontal과 Vertical정보를 표현하지 못한다는 점 입니다. 그림에서 보면, 초록색이 gt고, 빨간색이 predict된 상자입니다. 이때 위 그림은 predicted box가 아래로 내려간 경우, 오른쪽으로 간 경우, 중앙에 있는 경우를 표현하고 있습니다. 이 세경우는 GIOU가 모두 0.75로, GIOU는 거리에 대한 표현만 가능할 뿐이지, Vertical과 Horizontal에 대한 정보를 담지 못함을 보여줍니다. 본 논문에서는 이러한 문제를 DIOU를 제안함으로써 해결하고자 하였습니다. 

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure15.png?raw=true" alt="Figure15">

DIOU의 아이디어는 심플합니다. 기존의 GIOU에서 상자간의 거리를 통해 표현했던 C의 영역을 거리 지표로 사용하던 개념을 직접적인 거리 Measure인 Euclidean Distance로 바꿔보자는 개념입니다. 이를 통해 상자의 Area의 증감이 방향성을 갖는 Distance의 개념으로 바뀜으로써 Vertical과 Horizontal의 개념을 표현할 수 있게 됩니다. 이러한 DIOU는 기본 GIOU에서 Panalty Term을 새로 정의함으로써 구현되는데, 수식은 아래와 같습니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure16.png?raw=true" alt="Figure16">

수식에서 $R$은 새로 추가된 Panalty Term이고, L은 R을 이용한 DIOU Loss입니다. 이때 $p$는 Euclidean Distance를 의미하고, $c$는 기존의 GIOU에서 다뤘던 개념과 같습니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure17.png?raw=true" alt="Figure17">

본 논문에서는 DIOU의 성능을 실험하기 위해서 위와 같은 시뮬레이션을 수행하였습니다. 시뮬레이션은 10x10의 box 5000개를 만들고 각 박스에 대한 앵커를 7개의 aspect ratio로 할당해주었습니다. 이후 gt와 box들간의 loss를 구하여, 학습시켰을때의 결과가 오른쪽입니다. Iteration이 증가함에따라 error가 낮아지는것을 확인할 수 있는데, 기존 IOU에 비해 GIOU,DIOU가 성능이 좋은것을 볼 수 있습니다. 이때 GIOU보다 DIOU와 CIOU가 성능이 더 좋은것을 볼 수 있는데, CIOU에 대해서는 뒤에서 다시 다루도록 하겠습니다. 여기서 중요한점은 DIOU가 단순히 Horizontal과 Vertical의 정보를 표현하여 좀 더 정확해진 점 뿐만 아니라 GIOU의 Slow Convergence 또한 개선했음을 보여줍니다.

## Non-Maximum Suppression using DIoU

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure18.png?raw=true" alt="Figure18">

위의 수식은 본 논문에서 제안하는 DIOU-NMS의 표현으로 기존 NMS의 한계를 개선하고자 제안되었습니다. NMS는 IOU Metric을 사용하여 redundant detection boxes를 줄이는 역할을 합니다. 이때 NMS는 겹쳐진 객체들에 관해서 IOU만을 사용하면 False Suppression이 발생함을 지적하고, 이를 DIOU를 통해서 해결하고자 합니다. DIOU는 기존의 IOU에서 오버랩되는 영역뿐만 아니라 Center Point를 이용한 거리 또한 사용하기 때문에 suppression criterion을 좀 더 고려한다고 주장하고있습니다. 이때 $\epsilon$은 IOU Threshold를 뜻하고, $M$은 신뢰도 높은 Predicted Box를 의미합니다. $B_{i}$는 GT를 의미하며, $s_{i}$는 classification socre를 의미합니다. 

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure19.png?raw=true" alt="Figure19">

위 그림은 DIOU의 확장버전이 CIOU를 기반으로 학습시킨 Yolo v3를 바탕으로 NMS를 수행한 결과입니다. 좌측이 일반 NMS고 우측이 DIOU-NMS입니다. 보면 겹치는 Motor bike에 대해 DIOU-NMS는 잘 잡고 있는것을 확인할 수 있습니다. 그럼 다음장에서는 CIOU에 대해서 다뤄보도록 하겠습니다.

Complete IOU(CIOU)
=============
본 포스팅에서는 CIOU와 DIOU의 섹션을 나누어 설명했지만, 사실 DIOU논문에서 DIOU와 CIOU가 한번에 제안되었습니다. 본 논문의 저자는 DIOU를 통해 Horizontal 정보와 Vertical 정보를 표현하는데 성공하였습니다. 다만 저자는 더 나아가 다른 Geometric Measure또한 고려되어야 한다고 주장합니다. CIOU는 이러한 여러 Geometric Measure를 표현하고있는데, DIOU가 표현하는 Overlap Area, Central Point Distance 그리고 나아가 Aspect Ratio를 추가합니다. 

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure20.png?raw=true" alt="Figure20">

수식은 CIOU의 Regularize term으로 좌측은 기존의 DIOU고 우측에 Aspect Ratio를 반영하고있는 $\alpha$$v$가 추가된 형태입니다. 이때 $\alpha$는 positive value로 trade-off paramter입니다. $v$는 consistency of aspect ratio의 measure입니다. 이때 $v$는 gt의 ratio의 $arctan$값과 예측된 box의 ratio의 $arctan$값의 차이로 구해집니다. 이때 $\alpha$는 $v/(1-IOU) + v$로 non-overlapping case에서 overlap area factor가 regression loss에 더 높은 우선순위를 갖게 해주는 역할을 합니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure21.png?raw=true" alt="Figure21">

위 그림은 $L_{IOU}$, $L_{GIOU}$ 그리고 $L_{CIOU}$의 error를 시각화한 결과입니다. $L_{IOU}$의 경우에는 Non-overlapping case에 대해 높은 Loss를 보입니다. 그에비해 $L_{GIOU}$의 경우 Horizontal & Vertical case에 대해서만 높은 에러를 갖고 있습니다. $L_{CIOU}$의 경우는 전체적으로 안정된 에러율을 보이는것을 확인할 수 있습니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure22.png?raw=true" alt="Figure22">

위 테이블은 SSD와 Faster-RCNN에 GIOU,DIOU,CIOU를 적용했을때의 Ablation Study 결과입니다. 좌측은 VOC에서의 결과이고, 우측은 COCO에서의 결과입니다. 먼저 좌측을 보면 기법을 하나씩 적용할때마다 정확도가 향상되는것을 볼 수 있습니다. 또한 AP50일때와 AP75일때를 주목하여 보면 좀 더 Localization 능력이 향상되었음을 확인할 수 있습니다. 우측의 경우 Small, Medium, Large 사이즈 별로 확인할 수 있는데 DIOU에서 Small이 약간 향상됨을 볼 수 있고, 나머지 케이스에서는 기법들을 적용하였을때 더 좋은 효과를 볼 수 있습니다. 정성적 효과는 아래의 결과에서 볼 수 있습니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-11-12-IOU/Figure23.png?raw=true" alt="Figure23">

Conclusion
=============
여태까지의 객체 검출에서의 연구는 검출기의 아키텍처 변경을 통한 Feature Level의 성능향상이나 Focal Loss등을 통한 Classification에서의 성능 향상을 주로 다루었습니다. 이번 포스팅에서 IOU 와 IOU를 이용한 Loss Function의 발전에 대해 다뤘고, 이러한 연구 방향은 기존의 발전과는 다르게 Boundung Box Regression에서의 성능 향상을 보였다는 점에서 Novelty가 있습니다. 비록 $CIOU$에서는 Complete라는 표현을 사용하였지만, 앞으로 더 많은 발전 가능성이 있는 분야로 보입니다. 

Reference
=============
[1]Pham, Minh-Tri et al. “Fast polygonal integration and its application in extending haar-like features to improve object detection.” 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (2010): 942-949.<br>
[2]R. Von Gioi, J. Jakubowicz, J.-M. Morel, and G. Randall. Lsd: A fast line segment detector with a false detection control. PAMI, 32(4), 2010.<br>
[3]X. Sun, M. Christoudias, and P. Fua. Free-shape polygonal object localization. In ECCV, 2014.<br>
[4]K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask RCNN. In ICCV, 2017.<br>
[5]S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia. Path aggregation network for instance segmentation. In CVPR, 2018<br>
[6]Z. Li, J. Dirk Wegner, and A. Lucchi. Topological map extraction from overhead images. In ICCV, 2019.<br>
[7]YANG, Haichun, et al. CircleNet: Anchor-Free Glomerulus Detection with Circle Representation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2020. p. 35-44.<br>
[8]Rezatofighi, Hamid, et al. "Generalized intersection over union: A metric and a loss for bounding box regression." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>
[9]Zheng, Zhaohui, et al. "Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression." AAAI. 2020.<br>
[10]https://en.wikipedia.org/wiki/Jaccard_index<br>
