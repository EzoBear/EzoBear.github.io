---
layout: post
title: Real-time MOT(Multi-Object Tracker) 리뷰
date: 2020-09-01 15:36
category: Reatl-time, Object Trakcer
author: Ezobear
tags: [Object Tracker, Real-time Tracker, SDE, JDE, MOT, SOT]
excerpt: Object Detection과 마찬가지로 Object Tracking은 영상분야에서 전통적으로 연구되어온 분야 중 하나입니다. 과거에는 Tracking by Detection Strategy를 통해 Tracking 문제를 풀고자하였으나, 이러한 전략은 느리다는 구조적 한계를 가지고 있습니다. 따라서 최근에는 이러한 문제를 해결하고 Detection부터 Tracking까지 Single Shot에 가능한 Real-Time Tracker에 관한 연구가 진행되고있으며, 본 리뷰에서는 이러한 Real-Time Tracker에 초점을 맞추고자 합니다.
 
---
 
Introduction
=============
과거부터 지금까지 Object Tracking 문제는 이미지 분야에서 가장 활발히 연구되고 있는 분야 중 하나입니다. 따라서 많은 방법론들과 프레임워크들이 등장하였습니다. 이러한 Tracking 분야는 크게 Offline Tracking과 Online Tracking으로 나누어집니다. Offline Tracking은 과거, 현재, 미래의 모든 정보를 사용하여 객체를 추적하는 방법으로 Real-time 방법은 아닙니다. 따라서 본 리뷰에서는 Online Tracker위주로 다뤄보려고 합니다. 특히 Online Tracker들 중에서도 Multi-Object Tracking로 다뤄보도록 하겠습니다.
 
Tracking by Detection Strategy
=============
 
1장은 Object Tracker의 전통적인 전략인 Tracking by Detection 전략에 대해 다룹니다.
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure1.png?raw=true" alt="Figure 1">
 
위 그림[1]은 Object Tracker의 전통적인 전략인 Tracking by Detection에 대해 잘 보여줍니다. 처음 Video sequence가 있을때, 개별 프레임 단위로 Object Detection을 수행합니다. 이후 Object Detection에서 나온 결과로 각 Instance에 대한 정보(Bounding Box or Something)을 Object Tracker가 관리하게 됩니다. 이때 새로운 프레임이 들어오게 될 경우 다시 한번 Object Detection을 수행하고 Tracker가 관리하고있던 이전의 정보와 현재 Detection된 새로운 정보간의 Data Association을 측정하여 과거의 Detection되었던 객체와 현재의 객체를 매핑함으로써 Object Tracking을 수행합니다.
 
이때 연구 방향은 크게 3가지로 볼 수 있습니다. 첫째, 어떤 데이터를 바탕으로 Tracking할 것인가? 둘째, 어떻게 Association을 측정할 것인가? 셋째, Association을 바탕으로 Tracked Object에 대한 관리 전략을 어떻게 할것인가? 로 나눠질 수 있습니다. Tracker의 연구가 활발했던 만큼 이에 대한 연구도 많이되었는데, 본 리뷰에서는 첫번째와 두번째 위주로 다루겠습니다. 먼저 어떤 데이터를 측정할 것인가에 대하여 대표적인 몇가지 사례를 소개하고자합니다.
 
## 1. IOU Based Object Tracker
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure2.png?raw=true" alt="Figure 2">
 
IOU Based Object Tracker[2][3]는 Instance에 대한 정보를 IOU(Intersection over union)를 기반으로 하는 Tracker 입니다. IOU이란 두 영역의 교차영역의 넓이를 합영역의 값입니다. 즉 IOU가 높다는것은 Instance간의 겹치는 구간이 많다는 의미입니다. 동영상 프레임에서 어떤 객체는 현재 위치에서 많이 벗어나있을 가능성이 낮습니다. IOU Object Tracker는 이러한 점에 착안하여 현재 프레임에서 Detection된 객체는 다음 프레임에서 주변에 있을것이라 가정합니다. 이러한 가정은 대체적으로 잘 맞습니다. 따라서 굉장히 심플한 방법으로 좋은 성능을 낼 수 있습니다. 게다가 추가적인 학습이 필요하지 않다는 장점이 있습니다. 
 
하지만 위 그림 a)에서 보듯 한 객체가 Tracking되던 도중 Detection 과정에서 False Negative(미탐)이 발생할 경우 Fragmented를 통한 ID Swich문제가 발생한다는 점입니다. 연구[3]에서는 이러한 문제를 개선하고자 객체 추적이 Fragment된 순간부터 일정 프레임동안 Tracker의 정보를 Detection에 반영하는 방법(b)을 적용합니다. 그 후 일정 프레임 이상 Detection되지 않아 추적이 종료되었다고 판단하면, Tracker는 마지막으로 Fragment되었던 장소에서 Detection이 종료된 것으로 파악합니다. 
 
다만 이렇게 Tracker가 가지고 있는 정보를 다시 Detection에 반영하는 방법은 프레임이 늘어날수록 Tracking에 참가하고있는 객체의 양이 증가한다는 문제가 있습니다. 이러한 문제를 해결하기 위한 전략또한 같이 제시하지만, IOU Tracker의 근본적인 디자인 문제로 인해 이 문제의 완전한 해결은 어려워 보입니다. 
 
## 2. Feature Based Object Tracker
 본 장에서 소개하는 Tracker는 Feature Based Object Tracker로 Feature간의 상관성을 보는 방법을 사용합니다. 이러한 방법은 많은 연구가 되었던 만큼 어떤 Feature를 사용하고, 어떤 상관성을 어떻게 볼건지에 대한 이슈가 많지만, 본 리뷰에서는 다루지 않고 가장 기본적인 형태의 네트워크로 다루고 넘어가겠습니다. 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure3.png?raw=true" alt="Figure 3">
 
 그림에서 보듯 기본적으로 Feature based Tracker는 그림과 같이 Siamese 형태의 구조[5]를 갖고있습니다. Siames는 주로 이미지 서칭 연구에서 많이 사용되었었는데, 이는 두개의 입력 그림이 각각 네트워크로 들어가 Feature의 형태로 임베딩된 후 Feature 간의 유사성을 통해 같은 Image를 찾는 방법입니다. 
 
 Feature Based Object Tracker의 경우도 이러한 아이디어에 착안하여 설명할 수 있습니다. 위 그림은 Siames Network 구조를 가지고 있고, 입력은 크게 2가지로 구성됩니다. 첫번째는 내가 찾아야하는 Instance Target z입니다. z는 모델에 들어가 Feature의 형태로 임베딩됩니다. 두번째는 현재 Frame x로, z와 마찬가지로 모델을 통해 임베딩 Feature로 변환됩니다. 이 Feature는 입력사이즈가 다르기때문에 같은 모델을 거쳤음에도 사이즈가 다릅니다. 이러한 Feature는  cross-correlation layer를 통해 연관성에 대한 정보를 찾게되는데, cross-correlation 연산은 일종의 슬라이딩 윈도우의 개념으로 생각할 수 있습니다. 다만 전체 이미지 차원에서 슬라이딩 윈도우를 통한 Search가 많은 리소스를 소비하기때문에, 저차원으로 임베딩된 Feature에서의 슬라이딩 윈도우와, Feature Matching을 함으로써 객체를 Search하고, Tracking하는 것으로 생각할 수 있습니다. 이때 cross-correlation layer의 연산 결과는 낮은 차원의 1채널 Feature인데, 이는 작은 크기의 이미지로 생각할 수 있습니다. 이 이미지(output)가 가지고 있는 것은 Tracked 객체의 위치정보입니다. 즉, 현재 Frame x를 더 큰 Grid로 나누어 표현한것으로 볼 수 있습니다. Feature based Object Tracker는 이러한 방식을 통해 객체를 추적하게 됩니다. 이때 z는 Detector로부터 넘어오는 Detected 된 객체를 의미합니다. 이러한 방법은 Feature를 임베딩하는 방법과 corrlelation을 구하는 방법에 따라 성능이 달라지기 때문에 이에대한 연구가 많이 이루어지고 있지만, 여기서는 다루지 않겠습니다.
 
## 3. Optical Flow Based Object Tracker
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure18.png?raw=true" alt="Figure 4">
 
Optical Flow는 광학의 흐름으로 영상 프레임간 차연산의 확장버전으로 볼 수 있습니다. 따라서 어떤 물체가 이전에 비해 얼마나 움직였는지, 변화하였는지 얼마의 속도로 움직이고 있는지 등에 대한 정보를 담고있습니다. 이러한 Optical Flow는 기존의 루카스 카나데 알고리즘[28]을 통해 구할 수 있으며, 최근에는 이러한 Optical Flow 또한 딥러닝으로 구하고자 하는 시도[29][30]가 있습니다. 이러한 Optical Flow는 어떤 객체의 움직임을 담고 있는 정보이기 때문에, Tracking을 할때 유용하게 사용될 수 있습니다. 따라서 이러한 정보를 바탕으로 Tracking 혹은 Pose Estimation등에 대한 연구들이 과거부터 많이 수행되어 왔습니다[31][32][33][34]. 또한 최근에도 이러한 연구는 이어지고 있습니다[35][36][37]38]. 다만 Optical Flow Extractor에 의존적인 문제, 정지영상에서는 Optical Flow를 뽑을 수 없다는 점 등, Optical Flow를 뽑는데 추가적인 시간이 걸리는 등의 한계로인해 가장 최근 연구에서는 잘 보이지 않는 것 같습니다. 
 
위 소개한 Feature 외에도 Key Point와 그외 등의 Feature가 있지만, 그외 등은 잘 다뤄지지 않기때문에 여기서는 다루지 않겠습니다. 추가적으로 Key Point의 경우 최근 Tracker에서 자주 쓰이고 있고, 저도 중요하다고 생각하기 때문에 이후 섹션에서 보다 자세하게 따로 다루도록 하겠습니다.
 
## 4. Association Method
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure4.png?raw=true" alt="Figure 5">
세션 1,2,3이 어떤 데이터를 바탕으로 Tracking할지에 대한 연구였다면, 세션4는 어떤 방법으로 Assotiation을 만들어낼지에 대한 방법입니다. 다만 Assotiation을 만들어 내는 방법 크게 다양하지는 않습니다. 주로 칼만필터[6]를 이용하여 다음 프레임의 정보를 예측후, Hungarian Matching[7]을 통해 Optimal Solution을 찾아내는 방식을 사용하는데, 이때 다음 프레임의 정보는 Key Point, Appearance, Location 등 다양한 정보가 될 수 있습니다. 몇가지 연구[8][9][10]에서는 확률모델 기반이 아닌 RNN이나 LSTM을 이용한 딥러닝 모델을 사용하여 Assotiation 방법 또한 사용하고있으나, 원리 자체는 같습니다. 
 
위 그림[11]은 Deep Learning Model인 Sqe2Sqe Bi-RNN을 이용하여 Soft Association을 구하고 이를 통하여 Bipartite Matching을 수행하는 방법에 관한 그림입니다. 이는 특히 CVPR2020에 발표되었는데, MOT15와 MOT16에서 소타를 기록하는 등 Assotiation Method 또한 Tracker에서 중요한 연구 토픽임을 보여주었습니다. 다음장에서는 One Shot MOT에 대해 다루도록 하겠습니다.
 
이번 장에서는 Tracking by Detection Strategy를 갖는 MOT에 대해 다루었습니다. 이러한 전략을 갖는 MOT는 다른 말로 Two-Step 혹은 Two-Stage MOT라고도 부릅니다. 이러한 Two-Stage MOT는 디텍션에 대한 정보를 기반으로 트래킹을 하기때문에 반드시 디텍션 이후 트래킹을 수행하게 됩니다. 이러한 구조는 Bottleneck을 만들기 때문에 느리다는 한계를 갖습니다. 최근의 나온 2-Stage Tracker[12] 역시 정확도면에서는 좋은 성능을 보이지만 매우 느려서 Online Application에서는 사용하기 어렵다는 한계를 보입니다. 따라서 이러한 문제를 해결하기 위해 Tracking과 Detection을 동시에 하는 모델들이 연구되었는데, 이들이 Single(or One) Shot Object Tracker들입니다. 이러한 트래커들은 디텍션과 트래킹을 동시에 하기 때문에 매우 빠른 속도를 갖고 있으나, 2-Stage Tracker들 보다는 낮은 성능을 보였습니다만 최근에는 2-Stage와 비교해도 낮지 않은 성능을 갖은 모델들도 연구되고있습니다. 이에 대해서는 뒤에 기술하겠습니다.
 
One Shot MOT
=============
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure5.png?raw=true" alt="Figure 6">
 
위 그림[13]은 이러한 트래커들의 특징을 가장 잘 보여주는 그림입니다. (a)와 (b)는 Tracking by Detection 전략을 가져가는 2-Stage Tracker들의 그림입니다. 다만 Re-sampled feature를 cropped image를 쓸지 feature 그 자체를 쓸지에 대한 차이가 있습니다. (a)의 경우는 가장 전통적인 모델로 2번의 feature extraction이 발생하기때문에 (b)방식보다도 느립니다. (b)는 RPN에서 나온 feature를 sharing하기때문에 속도 측면에서 보다 더 개선된 모습을 보입니다. 그러나 여전히 2-Stage 방식은 Detection 정보를 기반으로 Tracking을 하기때문에 느리다는 한계가 있습니다. 따라서 본 연구[13]에서 제안하는 JDE방식은 Detection과 Tracking을 동시에 수행하여 이러한 Bottleneck을 제거합니다. 
 
## 1. Multi-Task Deep Learning
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure6.png?raw=true" alt="Figure 7">
 
One Shot MOT는 Multi-Task Deep Learning[14][15][16]의 발전과 관련이 많습니다. 일반적으로 딥러닝 모델은 어떠한 종류의 Task를 수행하기 위해 각각의 Task를 위한 모델을 구축하고 학습을 진행하게 됩니다. 이 경우 Task의 개수가 증가할때마다 저장해야할 학습 모델의 양은 증가하고, 저장용량 측면이나 유지보수 측면에서도 어려움이 생기게 됩니다. 이를 해결하기 위해 나온 연구들이 Multi-Task 딥러닝입니다.  이러한 멀티 테스크 딥러닝은 전제조건이 필요합니다. 여러개의 수행해야할 테스크들이 같은 Domain에 존재하여야 한다는 점 입니다. 예를들어 얼굴의 표정, 나이, 성별 등을 예측한다고 했을때, 이는 모두 얼굴이라는 도메인에 존재하는 데이터들을 통해 학습을 진행하고, 딥러닝 모델이 수행하는 테스크만 다른 이와 같은 경우가 Multi-Task입니다.
 
Tracking by Detection문제 또한 이러한 개념선상에서 생각할 수 있습니다. 찾아야할 객체, 트레킹해야할 객체는 같은 도메인 선상에 존재하는 심지어 같은 데이터입니다. 이러한 문제는 Multi-Task문제로 볼 수 있습니다. One Shot Tracker는 그림[16]의 universal feature extractor의 기본 개념을 확장한 버전으로 볼 수 있습니다. 같은 도메인상에 존재한 이미지내에서 Detection, Classification, Tracking을 모두 수행하는 딥러닝 모델을 구축함으로써 Tracking by Detection이 아닌 Tracking and Detection을 수행하게됩니다. 이를 통해 Bottleneck을 제거하고 빠른 Tracking을 수행할 수 있게됩니다. 
 
## 2. Object Tracker for Multi-Task Deep Learning(JDE)
 
어떻게보면, Multi-Task 딥러닝을 통해 오브젝트 트래커와 디텍션을 합친다는 아이디어는 심플하고, 쉬워보입니다. 다만 이제 어려운점은 그 방법에 관한점입니다. Tracking by Detection의 경우 Detection 정보를 통해 유사한 객체를 찾아다가 계속 추적하는 것으로 수행되니 참 직관적으로 이해할 수 있습니다. 다만 이를 한번에 수행하는 방법은 쉽게 떠올리기 어렵습니다. 이번 장에서는 Object Tracker가 어떻게 Detection과 Tracking을 동시에 수행하는지에 대해서 다루겠습니다.
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure7.png?raw=true" alt="Figure 8">
 
위 그림[13]은 Onshot Tracker의 아키텍처입니다. 좌측의 FPN은 Feature Extractor의 역할을 하고, 여기서 추출된 Feature를 바탕으로 Prediction Head가 Inference를 수행합니다. 이때 각각의 수행되는 작업은 Detection(Box Classification, Box Regression)과 Detection Box에 해당하는 Embedding Vector를 추출하는 것입니다. 이때 Embedding Vector는 각 Detection된 객체에 해당되는 Appearance Feature로 생각할 수 있습니다. 다시 MOT의 발전을 정리하자면, 기존의 Detection된 Box를 Crop하여, 다시 딥러닝 모델을 통해 Appearance Feature를 뽑고 이를 통해 유사성을 비교하던 SDE방식에서 RPN에서 Region Proposal된 Box와 Appearance Feature를 바로 사용하는 Two-Step방식, 그리고 마지막으로 동일한 Feature에서 위치, 카테고리, 인스턴스 별 구분가능한 고유의 특징까지 동시에 Multi-Task로 추출하 JDE방식으로 발전해왔습니다. 이는 다시 위 이전 섹션의 그림에서 확인할 수 있습니다. 
 
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure8.png?raw=true" alt="Figure 9">
 
그림[13] JDE가 Inference하는 Tensor들의 모양입니다. 이때 A는 Anchor의 개수, D는 Embedding Dimension입니다. 여기서 Detection Part는 RPN의 전략을 그대로 차용하여 H, W의 Map을 만들게되는데, 이는 이미지를 그리드로 나누었다고 생각할 수 있습니다. 따라서 H,W MAP상의 좌표가 실제 이미지에서의 인스턴스의 위치가됩니다. 그리고 Dense Embedding Map은 Appearance Map으로 각 바운딩 박스에 Corresponding하는 Feature의 정보를 담고있습니다. 이때 Appearance Feature의 경우 Online Matching의 Key가 되기 때문에 다른 물체와 구별할 수 있는 아이덴티티가 되어야합니다. 따라서 서로 동일한 객체와 다른 객체와의 거리는 멀수록 좋습니다. 이러한 문제에 대해서는 JDE에서는 기존의 연구[17]에서 제안한 Triplet Loss를 이용한 방법을 차용하여 적용합니다. 이때 Multi-Task Problem 에서의 각기 수행하는 Task의 Loss의 가중치를 정해주는 문제또한 중요한 문제입니다. JDE에서는 이 문제를 풀기 위해 학습 단계에서 Automatic Loss Balancing[18]를 적용하였습니다. 이는 각각의 독립적인 Loss를 learnable parameter로 가중하여 학습시킴으로써 uncertainty를 줄이는 방법입니다. 
 
## 3. Online Association Strategy for JDE
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure9.png?raw=true" alt="Figure 10">
 
JDE 연구에서 Online Association Strategy에는 크게 포커싱하지 않았기 때문에 기존의 심플하고 빠른 방법을 그대로 차용하여 사용합니다. 
주어진 비디오가 있을때, JDE 모델은 매 프레임별로 Detection을 수행하고, 각 바운딩 박스에 해당하는 임베딩 벡터를 출력합니다. 이때 임베딩 벡터 들과 이전 프레임에서 계산한 임베딩 벡터 풀에서의 Association을 구해 Matching을 수행하는데, 이는 널리 잘 알려진 Hungarian algorithm을 통해 수행합니다. 이를 통해 smooth trajectories와 location을 예측합니다. 만약에, assigned observation이 공간적으로 너무 먼 거리에 있으면 reject하여 참가 목록에서 제외합니다. 위 수식은 embedding 된 tracklet의 업데이트 룰로, f_t는 임베딩 tracklet을 뜻하며 (x, y, γ, h, x', y', γ', h')로 구성됩니다. 이떄 프라임들은 Kalman filter로 예측된 velocity를 뜻합니다. 추가로 t는 time stamp으로 현재 프레임을 뜻합니다. 에타는 스무싱 정도를 나타나는 모멘텀이고, JDE에서는 0.9를 사용하였습니다.
 
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure10.png?raw=true" alt="Figure 11">
 
JDE에서는 이러한 간단하고 빠른 방법을 통해 기존의 SOTA이자 Baseline Model로 많이 사용되던 SORT[19]의 Cascade Matching에 비해 빠르고, 정확한 성능을 보여줍니다. 그러나 JDE도 One Shot Tracker인 만큼 다른 Tracking by Detection 전략을 갖는 모델에 비해 속도는 빠르지만, 정확도가 좋지 않다는 한계가 있습니다. 다음장에서는 이러한 한계를 극복하기 위한 방법에 대해 다루겠습니다.
 
Advanced One Shot Object Tracker
=============
이전 장에서는 One Shot Object Tracker가 Multi-Task Deep Learning 기술들을 이용하여 Tracking Feature와 Detection Feature를 동시에 뽑고 Association을 하는 방식을 통해 기존의 Detection 정보를 통해 Tracking하는 전략보다 빠르게 추적을 수행하는 방법에 대해 알아보았습니다. 다만 이러한 방법은 속도가 빠른 대신에 정확도가 높지 않다는 한계를 가지고 있었는데요, 이번장에서는 왜 이러한 방식이 정확도가 높지 않을까? 라는 고민에서 시작되어 Tracking by Detection 전략만큼이나 정확한 One Shot Tracker를 연구개발한 논문인 A Simple Baseline for Multi-Object Tracking에 대해 다뤄보도록 하겠습니다. 이 논문[20]에서는 매우 간단한 방법으로 2-Stage Tracker만큼이나 정확하고 One Shot Tracker의 장점을 그대로 살려서 빠른 트래커를 만들었는데요, 사실 이 논문은 트래커의 프레임워크 개선을 통한 성능의 향상이 아니라 학습 방법, 아키텍처 모델링 및 추가적으로 디테일한 부분에서의 테크니컬한 방법을 통해 이를 해결함으로써 성능을 향상시켰습니다. 따라서 아직 개선하고 향상될 여지가 많은 Scaleability를 가졌기 때문에 많은 곳에서 Baseline으로 사용되기를 원해 저자들은 논문 이름을 저렇게 지었습니다. 또한 Tracker의 이름은  <a href = "https://github.com/ifzhang/FairMOT">Fair MOT</a>[21]로 짓기도 하였습니다. 이번 장은 저자들이 어떻게 테크니컬하게 성능을 향상시켰는지에 대해서 다뤄보도록 하겠습니다.
 
## Fair MOT Architecture
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure11.png?raw=true" alt="Figure 12">
 
그림 11[20]은 Fair MOT의 Architecture입니다. 이러한 Architecture적인 측면에서는 JDE와 거의 유사하나, 디테일한 테크닉측면에서 차이가 있습니다. 이러한 테크닉은 크게 3가지 관점에서 볼 수 있는데, 첫번째는 Anchor Free with Point Detection입니다. 본 연구에서는 JDE가 정확도가 낮은 이유를 분석하였고, Anchor가 학습을 방해한다는 것을 발견하였습니다. 따라서 Anchor Free를 채택하였는데요, 이때 Point Detection을 함으로써 높은 성능 향상을 가져왔습니다. 두번째는 Hourglass Structure입니다. 이러한 구조는 Auto Encoder구조로써도 볼 수 있습니다. 기존의 연구에서 이러한 구조가 잘 된다는 연구결과를 바탕으로 Feature Aggregation 테크닉을 적용하여 Hourglass Structure를 구성하였습니다. 그리고 세번째는 그 외의 디테일한 테크닉들입니다. 위에서 소개한 관점에서 각각 장에서는 좀 더 디테일하게 다뤄보도록 하겠습니다.
 
### Anchor Free with Key Point Detection
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure12.png?raw=true" alt="Figure 13">
 
그림[20]은 Fair MOT논문에서 발견한 Anchor가 Tracker 학습에 방해되는 상황에 대해 잘 표현해주고 있습니다. 좌측이 Anchor-based 방법이고 우측이 Anchor Free 방법입니다. 먼저 좌측을 보면 노란 상자와 빨간 상자를 볼 수 있습니다. 이 두개의 상자는 Anchor Box이고 이미지 상에서 파란 옷을 입은 선수에 해당하는 Anchor Box입니다. 분명 두 상자는 하나의 인스턴스를 가리키고있음에도 불구하고 실제 크롭해보면 굉장히 다른 모양을 보여주는데요, 이는 이를 기반으로 RE-ID를 뽑아낼때도 마찬가지로 다른 Feature를 뽑아내게 됩니다. 또한 노란 별과 빨간 별은 각각 Anchor의 중심부인데, 같은 인스턴스를 가리킴에도 cENTER또한 매우 다른 모습을 보여줍니다. 따라서 연구진들은 이러한 Anchor-based 접근이 Tracker의 성능을 떨어뜨린다고 생각하여 Anchor를 더이상 채용하지 않습니다. 그리고 Objects As Point 기법[22][23]을 이용하여 Anchor-Free를 도입함으로써 우측과 같이 오브젝트의 중심에 Box Center를 두게 됨으로써 정확한 Re-ID를 뽑을 수 있게됩니다. 이러한 Anchor Free를 통해 뽑아낸 방식의 경우 HeatMap과 Offset을 예측하기때문에, 추가적으로 Box Regression이후 NMS등의 Post-Processing이 필요 없고, 실제로 Box Level에서의 NMS를 사용하지 않기 때문에 겹치는 Object에 대해 검출을 잘 한다는 장점 또한 가지고 있습니다. 
 
여기서 다룬 Anchor Free의 경우 Key Point Detection과 연관이 있는데, Key Point Detection에서 중요한 ConnerNet[22]과 Objects As Detection[23]논문에 대해 간략하게 다루고 넘어가겠습니다.
 
* 여기서 Re-ID는 기존에 말하던 임베딩 벡터입니다. 즉 바운딩 박스에 매칭되는 인스턴스의 고유한 Appearance Feature입니다.
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure13.png?raw=true" alt="Figure 14">
 
ConnerNet은 Pose Estimation분야에서 Key Point Detection을 수행하는것에 영감을 얻어 만들어졌습니다. ConnerNet에서는 Key Point Estimation을 하는것을 응용하여 기존의 x,y,x,y의 4개의 Point를 예측하던것을 위 그림[22]과 같이 x,y에 x,y에 대한 offset을 예측함으로써 Detection 성능을 끌어올렸습니다. 이 논문에서 시사할점은 x,y,x,y의 4개의 Point를 예측하는것보다 포인트와 포인트로부터 offset을 예측하는것이 더 정확하다는 점입니다. 
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure14.png?raw=true" alt="Figure 15">
 
그림[22]는 ConnerNet의 Architecture입니다. ConnerNet에서는 point(x,y)와 offset을 예측하기 위해 estimator를 2개를 사용하였습니다. 하나는 Top-Left Corner를 예측하는 estimator고, 하나는 Bottom-Right를 예측하는 estimator입니다. 이렇게 estimator를 2개를 둠으로써 x,y와 각각의 offset을 예측할 수 있도록 네트워크를 구성하였습니다.
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure15.png?raw=true" alt="Figure 16">
 
그림은 Objects as Detection[23]의 아이디어입니다. ConnerNet에서 Point와 Offset을 예측하는것이 좋다는 것을 보여주었는데, 이는 2가지의 Point와 Offset을 예측해야합니다. 그보다는 하나의 점을 예측하고 그 점에대한 width,height offset을 계산한다면, 조금 더 계산과정이 단순해져서 좋지 않을까라는 아이디어에서 시작됩니다. 따라서 ConnerNet에서 2개의 점과 2개의 Offset을 예측하던 과정을 합침으로써 하나의점과 width, height offset을 예측함으로써 성능 향상을 보였습니다.
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure16.png?raw=true" alt="Figure 17">
 
또한 추가적으로 Center Point와 W,H Offset외의 다른 Factor들을 Estimation함으로써 성능향상을 하고자 하였는데, 사실상 Detection에서는 큰 성능이득은 없어보입니다. 다만 이를 응용하여 Tracker에 적용해본다면 좋은 연구가 될 것으로 보입니다.  
 
### Hourglass Structure
 
위의 장에서는 Anchor Free를 만듬으로써 디텍터와 트래커가 학습이 잘 되도록 만드는 것을 보았습니다. 추가적으로 그 원리인 Point Detection에 대해서도 간략하게 다뤘습니다. 본 장에서는 이러한 Task를 수행하는데 있어 가장 기본이되는 Feature를 뽑는 Feature Extractor에 대해 다뤄보도록 하겠습니다. 본 연구진들은 이러한 방법을 통해 스케일 변화를 처리하는 능력을 향상 시켜 identity switch를 문제를 개선했다고 주장했습니다.
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure17.png?raw=true" alt="Figure 18">
 
Pose Estiatmion분야에서 Hourglass Structure가 잘 동작한다는 것은 연구[24]에서 소개되었었습니다. 이러한 연구를 바탕으로 이 후 연구[25][26][27]에서도 이러한 구조를 많으 채용하여 사용했습니다. Pose Estimation의 핵심이 Key Point Detection이라 한다면, Key Point Detection중 하나인 Anchor Free 방법 또한 잘 동작하는것이 이상해보이지는 않습니다. 따라서 최근 이러한 Anchor Free의 Detector나 Tracker들은 모두 Hourglass Structure의 형태로 구성되는 경우가 많습니다. 본 장에서는 이러한 Hourglass Structure중 몇가지 좋아보이는 사례를 소개하고자 합니다. 
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure19.png?raw=true" alt="Figure 19">
 
그림은 Fair MOT[20]에서 사용한 Hourglass Structure입니다. 이는 Feature Aggregation[39] 논문에서 다룬 모델을 그대로 사용한 예제이며, 실제로도 Tracking에서의 높은 성능을 보이고 있습니다. 이러한 네트워크의 구조의 특징은 입력에서 Resolution을 유지한 채 진행되는 Branch를 기반으로 Down Sampling을 다양하게 수행하여 보다 Multi-Scale Feature를 다루고 있다는 점 입니다. 또한 이러한 Down Sampling된 Feature를 다시 Up-Scale하여 Resolution이 유지되는 Branch에 합쳐줌으로써 굉장히 제너럴하고 러프한 정보와 디테일한 정보가 합쳐진 Multi-Scale Feature를 만들어 내고 있습니다. 
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure20.png?raw=true" alt="Figure 20">
 
그림은 HRNET[40]의 구조로, Fair MOT 레포에 같이 있습니다. HRNET에서 HR은 High Resolution으로, 실제로 네트워크의 아웃풋을 보면 High Resolution임을 알 수 있습니다. 좀 더 자세하게 살펴보면, HRNET으로 들어온 입력은 Stem Block을 통해 Encoding이 되게 됩니다. 이렇게 Encoding된 Feature는 위의 Aggregation과 동일하게 Resolution을 유지하고 진행되는 Branch와 Down Sampling되었다가 다시 합쳐지는 Branch가 있습니다. 다만 Aggregation에서 제안한 구조보다 조금 더 복잡하게 Down Sample과 Upsample을 동시에하여 Weight를 공유하는 Feature가 좀 더 많게 구성됩니다. 이렇게 Multi-Scale Feature를 다루게되는데, 이후 이 Multi-Scale Feature는 Deconvolution되게 되는데, 이는 기존의 Human Pose Estimation과 Tracking에서 Baseline으로 사용되었던 연구[41]에서 제안한 방법을 사용하고있습니다. 이 외에도 레포를 살펴보면 Resnet-FPN[42]으로 Encoding하고 Deformable Convolution[43][44]으로 Decoding하는 Network또한 볼 수 있습니다. 위에 소개한 다른 네트워크에도 이와 같이 Deformable Convolution을 적용해보는것도 좋은 연구가 될 수 있을것으로 보입니다.
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure21.png?raw=true" alt="Figure 21">
 
그림은 Tracker는 아니고, FaceBook AI Research팀에서 발표한 Detection Framework인 DEtection with TRansformer(DETR)[45]의 아키텍처입니다. Hourglass Structure구조의 경우 사실 AutoEncoder구조입니다. 위에서 설명한 두개의 모델 또한 Encoding, Decoding이란 표현을 사용하고있습니다. 그런면에서 자연어처리 계열에서 많은 발전을 이룬 Auto Encoder의 확장버전인 Transformer를 적용해보는 것 또한 이상한 일은 아닙니다. 본 논문은 Transformer를 Detection에 적용한 첫 논문으로 의의를 가지고 있습니다. DETR은 Faster-RCNN과 같은 높은 정확도를 가지고 있습니다. 다만 Faster-RCNN 만큼 느리기도 합니다. 그럼에도 불구하고 Novelty를 갖는데, 첫째로는, NMS가 필요 없어 겹치는 Object에 대해 강인한것과, 둘째로는 순서 데이터(Positional Encoding)를 사용함에도 불구하고 병렬적으로 처리할 수 있기 때문에 RNN이나 LSTM과 같이 병목현상이 안생긴다는 점 입니다. 현재의 Object Tracker는 순서 정보나 시간 정보를 사용하지 않고있습니다. 따라서 이러한 정보를 사용할 수 있도록 Transformer를 적용해보는것도 좋은 연구가 될 수 있을것으로 보입니다. 
 
### Implementation Details of Fair MOT
이전 장에서는 One Shot Tracker가 2-Stage Tracker만큼이나 정확하게 학습될 수 있었던 테크닉들에 대해 Feature Extraction 방법과 이렇게 뽑은 Feature를 바탕으로 Detection & Tracking을 위한 정보 Estimation하는 방법에 다뤘습니다. 본 장에서는 Fair MOT에서 소개한 MOT 구현 및 학습에 사용되었던 디테일한 구현 정보에 대해 다루도록 하겠습니다.
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure8.png?raw=true" alt="Figure 8">
 
위의 그림은 JDE 때 사용했던 IO사이즈에 대한 그림입니다. Fair MOT는 JDE의 확장버전으로 볼 수 있습니다. 실제로 Association 부분은 JDE를 쓰며, 레포를 가봐도 JDE를 기반으로 만든 모델임을 알 수 있습니다. 따라서 여기서 어떻게 변화하였는지를 확인해보도록 하겠습니다. 
 
#### Backbone Network
일단 Backbone Network의 관점에서 보면 Fair MOT는 ResNet34를 기반으로한 DLA(Deep Layer Aggregation)을 기반으로 하고 있습니다. 이떄 Aggregation논문과는 차이점이 Aggreagation Part가 FPN 스타일로 구성된다는 점 입니다. 그리고 DeConvolution Module을 Deformable Convolution으로 사용했다는 점 입니다. 추가적으로 JDE와는 다르게 Fair MOT에서는 Anchor Free 방식을 채용하기때문에 Anchor를 사용하지 않아 N * H' * W'의 모양을 갖고, Heat Map의 경우 N이 1로 설정하여 한개의 Heat Map에 모든 오브젝트 Center를 표현하게 됩니다. 추가적으로 H', W'은 H/4, W/4 실제 입력 크기의 4분의 1의 크기의 Grid Map을 사용합니다. 
 
#### Re-ID(Embedded Appearance Feature)
기존 Tracker Model의 Re-ID Feature의 경우 High Dimensional Feature로 구성되어 왔습니다. 실제로 이러한 설계는 밴치마크와 챌린지에서 좋은 성능을 보여왔습니다. 하지만 본 논문에서는 데이터셋 관점에서 Re-ID 데이터는 Cropped Person Images만을 제공하기때문에 Fair MOT에서는 Re-ID 데이터를 사용할 수 없습니다. 때문에 실제 Re-ID보다 Training Image가 더 적어지게되는데, 이러한 이유로 Small Data에 대한 over-fitting이 발생하게됩니다. 따라서 Fair MOT에서는 이러한 문제를 Re-ID에 Low Dimensionality를 적용함으로써 해결하게 됩니다. 실제로 Fair MOT에서는 위 그림의 3번에서 dense embedding map(Re-ID)의 D(Demesion)을 128로 고정하게 됩니다. 이를 통해 Fair MOT에서는 좀 더 Robust한 MOTracker를 Build하였다고 주장합니다. 다만 이는 Small Data일때만 해당되고, 데이터 수가 증가할 경우 이와같은 효과는 점점 감소한다고 설명하였습니다. 
 
* 여기서 Re-ID는 기존에 말하던 임베딩 벡터로 바운딩 박스에 매칭되는 인스턴스의 고유한 Appearance Feature입니다. 즉 여기서 Re-ID의 Embedding Vector가 128이라는 것은 이미지 내에서 최대 객체의 개수가 128개 이하로 존재해야하는 것으로 보입니다(이는 추측입니다. 혹시 정확히 아시면 댓글 부탁드립니다.) 다만 실제 COCO 기준으로 NMS에서도 Candidate수를 100이하로 잡는 경우가 많습니다. 이는 128만 되더라도 COCO기준에서는 큰 문제가 되지 않음을 볼 수 있습니다. 만약에 실제 도메인에서 더 많은 검출이 필요할 경우 Re-ID의 Feature의 Dimension도 같이 키워줘 할 것으로 보입니다. 
 
### Loss Function
위의 장에서는 실제 Fair MOT구현을 위한 Detilas를 다뤘다면 본 장에서는 어떻게 Fair Mot를 학습시키는지에 대해 다루도록 하겠습니다. Fair MOT는 크게 4가지 정보를 Estimation하게 됩니다. 다만 Box Size와 Center Offset의 경우 한번에 Estimation되기 때문에 Loss Function 또한 Integrated되어 3 Term으로 구성되는데, 아래에서는 이 3개의 Loss에 대해 다뤄보도록 하겠습니다.
 
#### Heatmap Loss
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure22.png?raw=true" alt="Figure 22">
 
Heatmap Loss는 Point Detection을 기반으로 하기 때문에 x,y,x,y가 아닌 cx, cy, w, h로 계산되게 됩니다. 이때 아까 아웃풋의 크기는 Stride 4, 즉 입력 이미지에 4분의 1을 갖기 때문에 기에 대한 보정으로 Center값에 4를 나눠주게 됩니다. 이는 t-sne와 test를 통해 결정된 값으로 자세한 정보는 시각화 정보 및 성능 정보는 논문에서 확인할 수 있습니다. 또한 Mxy는 x,y에 대한 heatmap response인데, 이때 N은 Object의 개수입니다. 또한 시그마는 Standard deviation입니다. 이때 Loss는 pixel-wise logistic regression을 수행하는데, 이는 각 픽셀에 대해서 0~1까지의 확률값으로 표현하게 만듭니다. 따라서 이를통해 Heatmap을 구성할 수 습니다. 또한 여기에 focal loss를 적용하였습니다.
 
#### Offset & Size Loss
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure23.png?raw=true" alt="Figure 23">
 
위 수식은 Offset과 Size에 관련된 Loss입니다. Fair MOT 아키텍처상에서는 따로 예측하는것으로 나눠뒀지만 실제로는 두개는 합쳐서 학습하며, Estimation또한 합쳐져서 수행됩니다. Heatmap과 마찬가지로 Stride값으로 인해 입력이미지의 4분의 1의 크기의 Feature로 진행되며, Offset과 Size를 동시에 Regression하기 때문에 R은 W,H,2가 됩니다. 이때 Offset은 GT와의 거리값이고, 두개의 Loss는 l1 Loss로 구해집니다.
 
#### Re-ID Embedding Loss
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure24.png?raw=true" alt="Figure 24">
 
위 그림은 Identity Embedding Loss입니다. Fair MOT에서는 RE-ID를 추출하는 과정을 Classification Task로서 다룹니다. 특히 학습 데이터 세트에서 같은 Re-ID의 모든 Instance는 하나의 클래스로 다룹니다. 이때 이미지의 각 gt box b에 대하여 heatmap 좌표를 얻을 수 있는데, 이를 기반으로 identity feature vector E를 추출하여 class distribution vector p(k)와 매핑합니다. GT Class Label인 One-Hot Representation을 Li(k)로 표시하게되는데, 여기서 k는 Class의 개수입니다. 또한 이를 기반으로 softmax 계산하여 Loss로 사용합니다. 이를 다시 정리하면 Heatmap에서의 위치에 해당하는 위치에서 추출된 Feature Map인 E와 매핑되는 Class의 Feature Distribution을 기반으로 Softmax를 수행함으로써 Classification을 수행하는것으로 볼 수 있습니다. 
 
### Inference for Online Tracking
이전 섹션에서는 네트워크를 학습시킬때의 단계였다면 본 장에서는 Inference Phase에서의 Implementation Details를 보도록 하겠습니다. 본 장은 크게 Network관점에서의 Inference Details와 Oline Box Linking관점에서의 Details로 볼 수 있습니다.
 
#### Network Inference 
본 Fair Mot는 전작인 JDE와 동일하게 입력으로 1088 × 608 크기의 이미지를 받습니다. 이후 Heat map과 Heat map score를 기반으로 NMS(Nom-maximum suppression)을 수행하게됩니다. 위에서 Box-based NMS를 사용하지 않는것이 Anchor Free의 장점이라 하였는데, 여기서 NMS를 사용한다해서 햇갈릴수도 있으나, 여기서의 NMS는 Key-Point Based NMS[46]를 뜻합니다. 이는 Heat Map을 이용하여 Key Point를 Extraction하는데 사용됩니다. 그 후 Heat Map Score를 이용하여 일정 Threshold보다 높은 Key Point만을 유지하고 작은 Key Point들은 제거합니다. 그리고 추정된 Key Point에 Bounding Box의 크기와 Offset을 바탕으로 실제 Bounding Box를 계산하여 Detection을 수행합니다. 이후 추정된 Key Point는 Object Center로 볼 수 있는데, 이를 바탕으로 Identity Embedding(Re-ID)를 추출합니다. 이후 이 Re-ID는 Tracking 작업에 사용됩니다.
 
#### Online Box Linking 
Fair Mot에서는 위의 Inference를 바탕으로 뽑아낸 Identity Embedding을 가지고 Online Tracking을 수행하는데, 이는 전통적으로 사용하는 표준 방법과 크게 다르지는 않습니다. 여기서 Online Box Linking은 Data Association을 말합니다. 일단 첫번째 Frame에서 Detection을 수행하였고, 이를 기반으로 Tracklet을 초기화합니다. 이후 다음 입력이 들어왔을때 현재 프레임에서 다시 Detection을 수행하고, Re-ID와 IOU로 측정한 Distance를 바탕으로 Box를 기존 Tracklet에 Linking시킵니다. 또한 Kalman Filter를 기반으로 현재 프레임에서 Tracklet의 Location을 추정합니다. 이때 Linked된 Detection 결과에서 거리가 너무 멀면 Cost 를 무한대로 셋팅합니다. 이는 큰 모션과 함께 링킹된 Detection 결과를 효과적으로 예방하는데 도움이 된다고합니다. 이후 Re-ID에서 Appearance Variation 을 처리하기 위하여 각 time step 에서 Tracker의 Appearance Feature 를 업데이트 하는 과정을 반복합니다. 다음 섹션에서는 이와 같이 구성된 Fair MOT의 성능에 대해 확인해보도록 하겠습니다.
 
### Performance 
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure25.png?raw=true" alt="Figure 25">
 
위 표는 전작은 JDE와의 비교표입니다. MOTA는 Accuracy고, IDF1은 ID에 대한 F1-Score입니다. IDs는 스위칭 횟수가 FPS는 초당 Frame 처리 능력입니다. 이때 각 메저 옆에 화살표는 높으면 좋은 메저는 위로가있고, 낮으면 좋은 메저는 아래로 향해있습니다. 이때 정확도에 해당하는 MOTA는 MOT15 Train에서 약 10%정도가 향상된 것을 확인할 수 있습니다. 또한 IDs는 2.7배정도 허락된 것을 볼 수 있습니다. 이는 논문에서 주장하는 바를 잘 반영하고 있다고 볼 수 있습니다. 또한 FPS도 8FPS나 개선되어 Real-time인 30fps를 달성했습니다. 이때 그래픽카드는 RTX 2080 사용하고 CPU에 대한 언급은 따로 나와있지 않습니다. 
 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure26.png?raw=true" alt="Figure 26">
 
위 표는 two-step Tracker들과의 비교입니다. 본 논문에서는 Detection을 직접 수행하였기 때문에 private detector 프로토콜하에 실험이 진행되었습니다. 여기서 HZ는 Computational time을 뜻합니다. 다만 two-step tracker의 경우 detection 시간을 제외하고 tracker의 association time만을 기록한 것입니다.  그리고 본 논문에서 제안하는 One-Shot Tracker의 경우는 Detection과 Tracking 시간을 모두 기록한 것이기 때문에 이를 감안하고 성능 비교를 해야합니다. 여기서 Tracker 이름 오른쪽에 *이 붙어있는것들이 One-Shot Tracker이고 나머지는 two-stage tracker입니다. 위 표에서 볼 수 있듯이 Fair MOT는 two-stage tracker에 비해 정확도 또한 뒤쳐지지 않으며, 오히려 SOTA를 보이고 있습니다. 추가적으로 속도도 매우 빠르기때문에 Tracking의 새로운 Baseline으로써 자리잡을 수 있지 않을까 합니다.
 
Discussion
=============
본 리뷰에서는 Object Tracker의 발전과정에 대해서 다루고, Baseline인 JDE와 최근 새롭게 Baseline 자리 잡은 Fair MOT에 대해서 다뤘습니다. 다만 이러한 One Shot Tracker의 경우 몇가지 아쉬운점이있습니다. 일단 Tracker는 동영상 프레임상에서 돌어가는 딥러닝 네트워크입니다. 이는 시간에 대한 정보 또한 데이터에 포함되는데, 이에 대한 정보를 활용하지 못하는 점이 아쉽습니다. 두번째는 첫번째에 연장선인데, Tracking정보를 Detection하는데 활용하지 못하는 것이 아쉽습니다. 전통적으로 Tracker는 Detector의 미탐과 오탐을 보정해줌으로써 Detector가 보다 Robust해지게 해주는 장점이 있었습니다. 이러한 아이디어에서 시작한 Detect to track and track to detect[47]연구는 Detector와 Tracker가 상호보완될 수 있도록 네트워크를 구성하는 연구를 하였습니다. 본 리뷰에서 소개한 One Shot Tracker들 또한 이런식으로 Tracker와 Detector가 상호보완될 수 있다면 성능향상을 이룰 수 있을 것이라 사료됩니다. 이러한 방향으로 연구해보는 것도 좋은 챌린지가 될 것같습니다.
 
마치며...
=============
Object Tracking 분야를 공부하기 위해 조사하면서 자주 언급되는 논문, 혹은 제가 중요하게 본 논문 위주로 Tracking 분야에 대해 리뷰해보았습니다. 저도 공부하면서 작성한 글이기 때문에 부족한 점이나 몇가지 중요한 논문이 빠져있을수도있습니다. 댓글 남겨주시면 공부해서 업데이트하도록 하겠습니다. 또한 누군가에게 튜토리얼 자료로 잘 사용되었으면 좋겠습니다. 감사합니다
 
Reference
=============
[1]LEAL-TAIXÉ, Laura. Multiple object tracking with context awareness. arXiv preprint arXiv:1411.7935, 2014.<br>
[2]WOJKE, Nicolai; BEWLEY, Alex; PAULUS, Dietrich. Simple online and realtime tracking with a deep association metric. In: 2017 IEEE international conference on image processing (ICIP). IEEE, 2017. p. 3645-3649.<br>
[3]BOCHINSKI, Erik; SENST, Tobias; SIKORA, Thomas. Extending IOU based multi-object tracking by visual information. In: 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). IEEE, 2018. p. 1-6.<br>
[4]BERTINETTO, Luca, et al. Fully-convolutional siamese networks for object tracking. In: European conference on computer vision. Springer, Cham, 2016. p. 850-865.<br>
[5]KOCH, Gregory; ZEMEL, Richard; SALAKHUTDINOV, Ruslan. Siamese neural networks for one-shot image recognition. In: ICML deep learning workshop. 2015.<br>
[6]Welch, G., Bishop, G., et al.: An introduction to the kalman filter, 1995<br>
[7]KUHN, Harold W. The Hungarian method for the assignment problem. Naval research logistics quarterly, 1955, 2.1‐2: 83-97.
[8]Fang, K., Xiang, Y., Li, X., Savarese, S.: Recurrent autoregressive networks for
online multi-object tracking. In: 2018 IEEE Winter Conference on Applications of
Computer Vision (WACV). pp. 466–475. IEEE, 2018<br>
[9]Mahmoudi, N., Ahadi, S.M., Rahmati, M.: Multi-target tracking using cnn-based
features: Cnnmtt. Multimedia Tools and Applications 78(6), 7077–7096 (2019)<br>
[10]Zhou, Z., Xing, J., Zhang, M., Hu, W.: Online multi-target tracking with tensorbased high-order graph matching. In: 2018 24th International Conference on Pattern Recognition (ICPR). pp. 1809–1814. IEEE (2018)<br>
[11]XU, Yihong, et al. How To Train Your Deep Multi-Object Tracker. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. p. 6787-6796.<br>
[12]Yu, F., Li, W., Li, Q., Liu, Y., Shi, X., Yan, J.: Poi: Multiple object tracking with high performance detection and appearance feature. In: European Conference on Computer Vision. pp. 36–42. Springer (2016)<br>
[13]WANG, Zhongdao, et al. Towards real-time multi-object tracking. arXiv preprint arXiv:1909.12605, 2019.<br>
[14]KOKKINOS, Iasonas. Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. p. 6129-6138.<br>
[15]RANJAN, Rajeev, et al. Deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition. U.S. Patent Application No 15/746,237, 2018.<br>
[16]REBUFFI, Sylvestre-Alvise; BILEN, Hakan; VEDALDI, Andrea. Efficient parametrization of multi-domain deep neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. p. 8119-8127.<br>
[17]VOIGTLAENDER, Paul, et al. MOTS: Multi-object tracking and segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2019. p. 7942-7951.<br>
[18]KENDALL, Alex; GAL, Yarin; CIPOLLA, Roberto. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 7482-7491.<br>
[19]BEWLEY, Alex, et al. Simple online and realtime tracking. In: 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016. p. 3464-3468.<br>
[20]ZHAN, Yifu, et al. A Simple Baseline for Multi-Object Tracking. arXiv preprint arXiv:2004.01888, 2020.<br>
[21]https://github.com/ifzhang/FairMOT<br>
[22]LAW, Hei; DENG, Jia. Cornernet: Detecting objects as paired keypoints. In: Proceedings of the European Conference on Computer Vision (ECCV). 2018. p. 734-750.<br>
[23]ZHOU, Xingyi; WANG, Dequan; KRÄHENBÜHL, Philipp. Objects as points. arXiv preprint arXiv:1904.07850, 2019.<br>
[24]NEWELL, Alejandro; YANG, Kaiyu; DENG, Jia. Stacked hourglass networks for human pose estimation. In: European conference on computer vision. Springer, Cham, 2016. p. 483-499.<br>
[25]YANG, Wei, et al. Learning feature pyramids for human pose estimation. In: proceedings of the IEEE international conference on computer vision. 2017. p. 1281-1290.<br>
[26]SU, Zhihui, et al. Cascade feature aggregation for human pose estimation. arXiv preprint arXiv:1902.07837, 2019.<br>
[27]SUN, Ke, et al. Deep high-resolution representation learning for human pose estimation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2019. p. 5693-5703.<br>
[28]BOUGUET, Jean-Yves, et al. Pyramidal implementation of the affine lucas kanade feature tracker description of the algorithm. Intel corporation, 2001, 5.1-10: 4.<br>
[29]DOSOVITSKIY, Alexey, et al. Flownet: Learning optical flow with convolutional networks. In: Proceedings of the IEEE international conference on computer vision. 2015. p. 2758-2766.
[30]ILG, Eddy, et al. Flownet 2.0: Evolution of optical flow estimation with deep networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. p. 2462-2470.
[31]YAMAMOTO, Shinya, et al. Realtime multiple object tracking based on optical flows. In: Proceedings of 1995 IEEE International Conference on Robotics and Automation. IEEE, 1995. p. 2328-2333.<br>
[32]SHIN, Jeongho, et al. Optical flow-based real-time object tracking using non-prior training active feature model. Real-Time Imaging, 2005, 11.3: 204-218.<Br>
[33]MIKIC, Ivana; KRUCINSKI, Slawomir; THOMAS, James D. Segmentation and tracking in echocardiographic sequences: Active contours guided by optical flow estimates. IEEE transactions on medical imaging, 1998, 17.2: 274-284.<br>
[34]CHEN, Zhiwen, et al. Tracking of moving object based on optical flow detection. In: Proceedings of 2011 International Conference on Computer Science and Network Technology. IEEE, 2011. p. 1096-1099.
[35]BEAUPRÉ, David-Alexandre; BILODEAU, Guillaume-Alexandre; SAUNIER, Nicolas. Improving multiple object tracking with optical flow and edge preprocessing. arXiv preprint arXiv:1801.09646, 2018.<br>
[36]KEUPER, Margret, et al. Motion segmentation & multiple object tracking by correlation co-clustering. IEEE transactions on pattern analysis and machine intelligence, 2018, 42.1: 140-153.<br>
[37]KEUPER, Margret, et al. Motion segmentation & multiple object tracking by correlation co-clustering. IEEE transactions on pattern analysis and machine intelligence, 2018, 42.1: 140-153.<br>
[38]WEN, Longyin, et al. Visdrone-mot2019: The vision meets drone multiple object tracking challenge results. In: Proceedings of the IEEE International Conference on Computer Vision Workshops. 2019. p. 0-0.<br>
[39]YU, Fisher, et al. Deep layer aggregation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 2403-2412.<br>
[40]CHENG, Bowen, et al. HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation. arXiv preprint arXiv:1908.10357, 2019.<br>
[41]XIAO, Bin; WU, Haiping; WEI, Yichen. Simple baselines for human pose estimation and tracking. In: Proceedings of the European conference on computer vision (ECCV). 2018. p. 466-481.<br>
[42]LIN, Tsung-Yi, et al. Feature pyramid networks for object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. p. 2117-2125.<br>
[43]DAI, Jifeng, et al. Deformable convolutional networks. In: Proceedings of the IEEE international conference on computer vision. 2017. p. 764-773.<br>
[44]ZHU, Xizhou, et al. Deformable convnets v2: More deformable, better results. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. p. 9308-9316.<br>
[45]CARION, Nicolas, et al. End-to-End Object Detection with Transformers. arXiv preprint arXiv:2005.12872, 2020.<br>
[46]PAPANDREOU, George, et al. Towards accurate multi-person pose estimation in the wild. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. p. 4903-4911.<br>
[47]FEICHTENHOFER, Christoph; PINZ, Axel; ZISSERMAN, Andrew. Detect to track and track to detect. In: Proceedings of the IEEE International Conference on Computer Vision. 2017. p. 3038-3046.<br>

