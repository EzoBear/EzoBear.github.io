---
layout: post
title: Real-time MOT(Multi-Object Tracker) 리뷰
date: 2020-09-01 15:36
category: Reatl-time, Object Trakcer
author: Ezobear
tags: [Object Tracker, Real-time Tracker, SDE, JDE, MOT, SOT]
excerpt: Object Detection과 마찬가지로 Object Tracking은 영상분야에서 전통적으로 연구되어온 분야 중 하나입니다. 과거에는 Tracking by Detection Strategy를 통해 Tracking 문제를 풀고자하였으나, 이러한 전략은 느리다는 구조적 한계를 가지고 있습니다. 따라서 최근에는 이러한 문제를 해결하고 Detection부터 Tracking까지 Single Shot에 가능한 Real-Time Tracker에 관한 연구가 진행되고있으며, 본 리뷰에서는 이러한 Real-Time Tracker에 초점을 맞추고자 합니다.

---

Introduction
=============
과거부터 지금까지 Object Tracking 문제는 이미지 분야에서 가장 활발히 연구되고 있는 분야 중 하나입니다. 따라서 많은 방법론들과 프레임워크들이 등장하였습니다. 이러한 Tracking 분야는 크게 Offline Tracking과 Online Tracking으로 나누어집니다. Offline Tracking은 과거, 현재, 미래의 모든 정보를 사용하여 객체를 추적하는 방법으로 Real-time 방법은 아닙니다. 따라서 본 리뷰에서는 Online Tracker위주로 다뤄보려고 합니다. 특히 Online Tracker들 중에서도 Multi-Object Tracking로 다뤄보도록 하겠습니다.

Tracking by Detection Strategy
=============

1장은 Object Tracker의 전통적인 전략인 Tracking by Detection 전략에 대해 다룹니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure1.png?raw=true" alt="Figure 1">

위 그림[1]은 Object Tracker의 전통적은 전략인 Tracking by Detection에 대해 잘 보여줍니다. 처음 Video sequence가 있을때, 개별 프레임 단위로 Object Detection을 수행합니다. 이후 Object Detection에서 나온 결과로 각 Instance에 대한 정보(Bounding Box or Something)을 Object Tracker가 관리하게 됩니다. 이떄 새로운 프레임이 들어올게 될 경우 다시 한번 Object Detection을 수행하고 Tracker가 관리하고있던 이전의 정보와 현재 Detection된 새로운 정보간의 Data Association을 측정하여 과거의 Detection되었던 객체와 현재의 객체를 매핑함으로써 Object Tracking을 수행합니다.

이때 연구 방향은 크게 3가지로 볼 수 있습니다. 첫째, 어떤 데이터를 바탕으로 Tracking할 것인가? 둘째, 어떻게 Assotiation을 측정할 것인가? 셋째, Assotiation을 바탕으로 Tracked Object에 대한 관리 전략을 어떻게 할것인가? 로 나눠질 수 있습니다. Tracker의 연구가 활발했던 만큼 이에 대한 연구도 많이되었는데, 본 리뷰에서는 첫번째와 두번째 위주로 다루겠습니다. 먼저 어떤 데이터를 측정할 것인가에 대하여 대표적인 몇가지 사례를 소개하고자합니다.

## 1. IOU Based Object Tracker
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure2.png?raw=true" alt="Figure 2">

IOU Based Object Tracker[2][3]는 Instnace에 대한 정보를 IOU(Intersection over union)를 기반으로 하는 Tracker 입니다. IOU이란 두 영역의 교차영역의 넓이를 합영역의 값입니다. 즉 IOU가 높다는것은 Instance간의 겹치는 구간이 많다는 의미입니다. 동영상 프레임에서 어떤 객체는 현재 위치에서 많이 벗어나있을 가능성이 낮습니다. IOU Object Tracker는 이러한 점에 착안하여 현재 프레임에서 Detection된 객체는 다음 프레임에서 주변에 있을것이라 가정합니다. 이러한 가정은 대체적으로 잘 맞습니다. 따라서 굉장히 심플한 방법으로 좋은 성능을 낼 수 있습니다. 게다가 추가적인 학습이 필요하지 않다는 장점이 있습니다. 

하지만 위 그림 a)에서 보듯 한 객체가 Tracking되던 도중 Detection 과정에서 False Negative(미탐)이 발생할 경우 Fragmented를 통한 ID Swich문제가 발생한다는 점입니다. 연구[3]에서는 이러한 문제를 개선하고자 객체 추적이 Fragment된 순간부터 일정 프레임동안 Tracker의 정보를 Detection에 반영하는 방법(b)을 적용합니다. 그 후 일정 프레임 이상 Detection되지 않아 추적이 종료되었다고 판단하면, Tracker는 마지막으로 Fragment되었던 장소에서 Detection이 종료된 것으로 파악합니다. 

다만 이렇게 Tracker가 가지고 있는 정보를 다시 Detection에 반영하는 방법은 프레임이 늘어날수록 Tracking에 참가하고있는 객체의 양이 증가한다는 문제가 있습니다. 이러한 문제를 해결하기 위한 전략또한 같이 제시하지만, IOU Tracker의 근본적인 디자인 문제로 인해 이 문제의 완전한 해결은 어려워 보입니다. 

## 2. Feature Based Object Tracker
 본 장에서 소개하는 Tracker는 Feature Based Object Tracker로 Feature간의 상관성을 보는 방법을 사용합니다. 이러한 방법은 많은 연구가 되었던 만큼 어떤 Feature를 사용하고, 어떤 상관성을 어떻게 볼건지에 대한 이슈가 많지만, 본 리뷰에서는 다루지 않고 가장 기본적인 형태의 네트워크로 다루고 넘어가겠습니다. 
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure3.png?raw=true" alt="Figure 3">

 그림에서 보듯 기본적으로 Feature based Tracker는 그림과 같이 Siamese 형태의 구조[5]를 갖고있습니다. Siames는 주로 이미지 서칭 연구에서 많이 사용되었었는데, 이는 두개의 입력 그림이 각각 네트워크로 들어가 Feature의 형태로 임베딩된 후 Feature 간의 유사성을 통해 같은 Image를 찾는 방법입니다. 
 
 Feature Based Object Tracker의 경우도 이러한 아이디어에 착안하여 설명할 수 있습니다. 위 그림은 Siames Network 구조를 가지고 있고, 입력은 크게 2가지로 구성됩니다. 첫번째는 내가 찾아야하는 Instance Target z입니다. z는 모델에 들어가 Feature의 형태로 임베딩됩니다. 두번째는 현재 Frame x로, z와 마찬가지로 모델을 통해 임베딩 Feature로 변환됩니다. 이 Feature는 입력사이즈가 다르기때문에 같은 모델을 거쳤음에도 사이즈가 다릅니다. 이러한 Feature는  cross-correlation layer를 통해 연관성에 대한 정보를 찾게되는데, cross-correlation 연산은 일종의 슬라이딩 윈도우의 개념으로 생각할 수 있습니다. 다만 전체 이미지 차원에서 슬라이딩 윈도우를 통한 Search가 많은 리소스를 소비하기때문에, 저차원으로 임베딩된 Feature에서의 슬라이딩 윈도우와, Feature Matching을 함으로써 객체를 Search하고, Tracking하는 것으로 생각할 수 있습니다. 이때 cross-corrlelation layer의 연산 결과는 낮은 차원의 1채널 Feature인데, 이는 작은 크기의 이미지로 생각할 수 있습니다. 이 이미지(output)가 가지고 있는 것은 Tracked 객체의 위치정보입니다. 즉, 현재 Frame x를 더 큰 Grid로 나누어 표현한것으로 볼 수 있습니다. Feature based Object Tracker는 이러한 방식을 통해 객체를 추적하게 됩니다. 이때 z는 Detector로부터 넘어오는 Detected 된 객체를 의미합니다. 이러한 방법은 Feature를 임베딩하는 방법과 corrlelation을 구하는 방법에 따라 성능이 달라지기 때문에 이에대한 연구가 많이 이루어지고 있지만, 여기서는 다루지 않겠습니다.

## 3. Association Method
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure4.png?raw=true" alt="Figure 4">
세션 1,2가 어떤 데이터를 바탕으로 Tracking할지에 대한 연구였다면, 세션3은 어떤 방법으로 Assotiation을 만들어낼지에 대한 방법입니다. 다만 Assotiation을 만들어 내는 방법 크게 다양하지는 않습니다. 주로 칼만필터[6]를 이용하여 다음 프레임의 정보를 예측후, Hungarian Matching[7]을 통해 Optimal Solution을 찾아내는 방식을 사용하는데, 이때 다음 프레임의 정보는 Key Point, Appearance, Location 등 다양한 정보가 될 수 있습니다. 몇가지 연구[8][9][10]에서는 확률모델 기반이 아닌 RNN이나 LSTM을 이용한 딥러닝 모델을 사용하여 Assotiation 방법 또한 사용하고있으나, 원리 자체는 같습니다. 

위 그림[11]은 Deep Learning Model인 Sqe2Sqe Bi-RNN을 이용하여 Soft Association을 구하고 이를 통하여 Bi-Partite Matching을 수행하는 방법에 관한 그림입니다. 이는 특히 CVPR2020에 발표되었는데, MOT15와 MOT16에서 소타를 기록하는 등 Assotiation Method 또한 Tracker에서 중요한 연구 토픽임을 보여주었습니다. 다음장에서는 One Shot MOT에 대해 다루도록 하겠습니다.

이번 장에서는 Tracking by Detection Strategy를 갖는 MOT에 대해 다루었습니다. 이러한 전략을 갖는 MOT는 다른 말로 Two-Step 혹은 Two-Stage MOT라고도 부릅니다. 이러한 Two-Stage MOT는 디텍션에 대한 정보를 기반으로 트래킹을 하기때문에 반드시 디텍션 이후 트래킹을 수행하게 됩니다. 이러한 구조는 Bottleneck을 만들기 때문에 느리다는 한계를 갖습니다. 최근의 나온 2-Stage Tracker[12] 역시 정확도면에서는 좋은 성능을 보이지만 매우 느려서 Online Application에서는 사용하기 어렵다는 한계를 보입니다. 따라서 이러한 문제를 해결하기 위해 Tracking과 Detection을 동시에 하는 모델들이 연구되었는데, 이들이 Single(or One) Shot Object Tracker들입니다. 이러한 트래커들은 디텍션과 트래킹을 동시에 하기 때문에 매우 빠른 속도를 갖고 있으나, 2-Stage Tracker들 보다는 낮은 성능을 보였습니다만 최근에는 2-Stage와 비교해도 낮지 않은 성능을 갖은 모델들도 연구되고있습니다. 이에 대해서는 뒤에 기술하겠습니다.

One Shot MOT
=============
<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure5.png?raw=true" alt="Figure 5">

위 그림[13]은 이러한 트래커들의 특징을 가장 잘 보여주는 그림입니다. (a)와 (b)는 Tracking by Detection 전략을 가져가는 2-Stage Tracker들의 그림입니다. 다만 Re-sampled feature를 cropped image를 쓸지 feature 그 자체를 쓸지에 대한 차이가 있습니다. (a)의 경우는 가장 전통적인 모델로 2번의 feature extraction이 발생하기때문에 (b)방식보다도 느립니다. (b)는 RPN에서 나온 feature를 sharing하기때문에 속도 측면에서 보다 더 개선된 모습을 보입니다. 그러나 여전히 2-Stage 방식은 Detection 정보를 기반으로 Tracking을 하기때문에 느리다는 한계가 있습니다. 따라서 본 연구[13]에서 제안하는 JDE방식은 Detection과 Tracking을 동시에 수행하여 이러한 Bottleneck을 제거합니다. 


## 1. Multi-Task Deep Learning

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure6.png?raw=true" alt="Figure 6">

One Shot MOT는 Multi-Task Deep Learning[14][15][16]의 발전과 관련이 많습니다. 일반적으로 딥러닝 모델은 어떠한 종류의 Task를 수행하기 위해 각각의 Task를 위한 모델을 구축하고 학습을 진행하게 됩니다. 이 경우 Task의 개수가 증가할때마다 저장해야할 학습 모델의 양은 증가하고, 저장용량 측면이나 유지보수 측면에서도 어려움이 생기게 됩니다. 이를 해결하기 위해 나온 연구들이 Multi-Task 딥러닝입니다.  이러한 멀티 테스크 딥러닝은 전제조건이 필요합니다. 여러개의 수행해야할 테스크들이 같은 Domain에 존재하여야 한다는 점 입니다. 예를들어 얼굴의 표정, 나이, 성별 등을 예측한다고 했을때, 이는 모두 얼굴이라는 도메인에 존재하는 데이터들을 통해 학습을 진행하고, 딥러닝 모델이 수행하는 테스크만 다른 이와 같은 경우가 Multi-Task입니다.

Tracking by Detection문제 또한 이러한 개념선상에서 생각할 수 있습니다. 찾아야할 객체, 트레킹해야할 객체는 같은 도메인 선상에 존재하는 심지어 같은 데이터입니다. 이러한 문제는 Multi-Task문제로 볼 수 있습니다. One Shot Tracker는 그림[16]의 universal feature extractor의 기본 개념을 확장한 버전으로 볼 수 있습니다. 같은 도메인상에 존재한 이미지내에서 Detection, Classification, Tracking을 모두 수행하는 딥러닝 모델을 구축함으로써 Tracking by Detection이 아닌 Tracking and Detection을 수행하게됩니다. 이를 통해 Bottleneck을 제거하고 빠른 Tracking을 수행할 수 있게됩니다. 

## 2. Object Tracker for Multi-Task Deep Learning(JDE)

어떻게보면, Multi-Task 딥러닝을 통해 오브젝트 트래커와 디텍션을 합친다는 아이디어는 심플하고, 쉬워보입니다. 다만 이제 어려운점은 그 방법에 관한점입니다. Tracking by Detection의 경우 Detection 정보를 통해 유사한 객체를 찾아다가 계속 추적하는 것으로 수행되니 참 직관적으로 이해할 수 있습니다. 다만 이를 한번에 수행하는 방법은 쉽게 떠올리기 어렵습니다. 이번 장에서는 Object Tracker가 어떻게 Detection과 Tracking을 동시에 수행하는지에 대해서 다루겠습니다.

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure7.png?raw=true" alt="Figure 7">

위 그림[13]은 Onshot Tracker의 아키텍처입니다. 좌측의 FPN은 Feature Extractor의 역할을 하고, 여기서 추출된 Feature를 바탕으로 Prediction Head가 Inference를 수행합니다. 이때 각각의 수행되는 작업은 Detection(Box Classification, Box Regression)과 Detection Box에 해당하는 Embedding Vector를 추출하는 것입니다. 이때 Embedding Vector는 각 Detection된 객체에 해당되는 Appearance Feature로 생각할 수 있습니다. 다시 MOT의 발전을 정리하자면, 기존의 Detection된 Box를 Crop하여, 다시 딥러닝 모델을 통해 Appearance Feature를 뽑고 이를 통해 유사성을 비교하던 SDE방식에서 RPN에서 Region Proposal된 Box와 Appearance Feature를 바로 사용하는 Two-Step방식, 그리고 마지막으로 동일한 Feature에서 위치, 카테고리, 인스턴스 별 구분가능한 고유의 특징까지 동시에 Multi-Task로 추출하 JDE방식으로 발전해왔습니다. 이는 다시 그림5에서 확인할 수 있습니다. 


<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure8.png?raw=true" alt="Figure 8">

그림[13] JDE가 Inference하는 Tensor들의 모양입니다. 이때 A는 Anchor의 개수, D는 Embedding Demension입니다. 여기서 Detection Part는 RPN의 전략을 그대로 차용하여 H, W의 Map을 만들게되는데, 이는 이미지를 그리드로 나누었다고 생각할 수 있습니다. 따라서 H,W MAP상의 좌표가 실제 이미지에서의 인스턴스의 위치가됩니다. 그리고 Dense Embedding Map은 Appearance Map으로 각 바운딩 박스에 Corresponding하는 Feature의 정보를 담고있습니다. 이떄 Appearance Feature의 경우 Online Matching의 Key가 되기 때문에 다른 물체와 구별할 수 있는 아이덴티티가 되어야합니다. 따라서 서로 동일한 객체와 다른 객체와의 거리는 멀수록 좋습니다. 이러한 문제에 대해서는 JDE에서는 기존의 연구[17]에서 제안한 Triplet Loss를 이용한 방법을 차용하여 적용합니다. 이때 Multi-Task Problem 에서의 각기 수행하는 Task의 Loss의 가중치를 정해주는 문제또한 중요한 문제입니다. JDE에서는 이 문제를 풀기 위해 학습 단계에서 Automatic Loss Balancing[18]를 적용하였습니다. 이는 각각의 독립적인 Loss를 learnable parameter로 가중하여 학습시키므로써 uncertainty를 줄이는 방법입니다. 

## 3. Online Association Strategy for JDE

<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure9.png?raw=true" alt="Figure 9">

JDE 연구에서 Online Association Strategy에는 크게 포커싱하지 않았기 때문에 기존의 심플하고 빠른 방법을 그대로 차용하여 사용합니다. 
주어진 비디오가 있을때, JDE 모델은 매 프레임별로 Detection을 수행하고, 각 바운딩 박스에 해당하는 임베딩 벡터를 출력합니다. 이떄 임베딩백터들과 이전 프레임에서 계산한 임베딩 벡터 풀에서의 Association을 구해 Matching을 수행하는데, 이는 널리 잘 알려진 Hungarian algorithm을 통해 수행합니다. 이를 통해 smooth trajectories와 location을 예측합니다. 만약에, assined observation이 공간적으로 너무 먼 거리에 있으면 reject하여 참가 목록에서 제외합니다. 위 수식은 embedding 된 tracklet의 업데이트 룰로, f_t는 임베딩 tracklet을 뜻하며 (x, y, γ, h, x', y', γ', h')로 구성됩니다. 이떄 프라임들은 Kalman filter로 예측된 velocity를 뜻합니다. 추가로 t는 time stamp으로 현재 프레임을 뜻합니다. 에타는 스무싱 정도를 나타나는 모멘텀이고, JDE에서는 0.9를 사용하였습니다.


<img src="https://github.com/EzoBear/EzoBear.github.io/blob/master/assets/images/post_images/2020-09-01-MultiObjectTracker-Review/Figure10.png?raw=true" alt="Figure 10">

JDE에서는 이러한 간단하고 빠른 방법을 통해 기존의 SOTA이자 Baseline Model로 많이 사용되던 SORT[19]의 Casecade Matching에 비해 빠르고, 정확한 성능을 보여줍니다. 그러나 JDE도 One Shot Tracker인 만큼 다른 Tracking by Detection 전략을 갖는 모델에 비해 속도는 빠르지만, 정확도가 좋지 않다는 한계가 있습니다. 다음장에서는 이러한 한계를 극복하기 위한 방법에 대해 다루겠습니다.

Advanced One Shot Object Tracker
=============
업데이트 예정...

마치며...
=============
Object Tracking 분야를 공부하기 위해 조사하면서 자주 언급되는 논문, 혹은 제가 중요하게 본 논문 위주로 Tracking 분야에 대해 리뷰해보았습니다. 저도 공부하면서 작성한 글이기 때문에 부족한 점이나 몇가지 중요한 논문이 빠져있을수도있습니다. 댓글 남겨주시면 공부해서 업데이트하도록 하겠습니다. 또한 누군가에게 튜토리얼 자료로 잘 사용되었으면 좋겠습니다. 감사합니다

Reference
=============
[1]LEAL-TAIXÉ, Laura. Multiple object tracking with context awareness. arXiv preprint arXiv:1411.7935, 2014.<br>
[2]WOJKE, Nicolai; BEWLEY, Alex; PAULUS, Dietrich. Simple online and realtime tracking with a deep association metric. In: 2017 IEEE international conference on image processing (ICIP). IEEE, 2017. p. 3645-3649.<br>
[3]BOCHINSKI, Erik; SENST, Tobias; SIKORA, Thomas. Extending IOU based multi-object tracking by visual information. In: 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). IEEE, 2018. p. 1-6.<br>
[4]BERTINETTO, Luca, et al. Fully-convolutional siamese networks for object tracking. In: European conference on computer vision. Springer, Cham, 2016. p. 850-865.<br>
[5]KOCH, Gregory; ZEMEL, Richard; SALAKHUTDINOV, Ruslan. Siamese neural networks for one-shot image recognition. In: ICML deep learning workshop. 2015.<br>
[6]Welch, G., Bishop, G., et al.: An introduction to the kalman filter, 1995<br>
[7]KUHN, Harold W. The Hungarian method for the assignment problem. Naval research logistics quarterly, 1955, 2.1‐2: 83-97.
[8]Fang, K., Xiang, Y., Li, X., Savarese, S.: Recurrent autoregressive networks for
online multi-object tracking. In: 2018 IEEE Winter Conference on Applications of
Computer Vision (WACV). pp. 466–475. IEEE, 2018<br>
[9]Mahmoudi, N., Ahadi, S.M., Rahmati, M.: Multi-target tracking using cnn-based
features: Cnnmtt. Multimedia Tools and Applications 78(6), 7077–7096 (2019)<br>
[10]Zhou, Z., Xing, J., Zhang, M., Hu, W.: Online multi-target tracking with tensorbased high-order graph matching. In: 2018 24th International Conference on Pattern Recognition (ICPR). pp. 1809–1814. IEEE (2018)<br>
[11]XU, Yihong, et al. How To Train Your Deep Multi-Object Tracker. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. p. 6787-6796.<br>
[12]Yu, F., Li, W., Li, Q., Liu, Y., Shi, X., Yan, J.: Poi: Multiple object tracking with high performance detection and appearance feature. In: European Conference on Computer Vision. pp. 36–42. Springer (2016)<br>
[13]WANG, Zhongdao, et al. Towards real-time multi-object tracking. arXiv preprint arXiv:1909.12605, 2019.<br>
[14]KOKKINOS, Iasonas. Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. p. 6129-6138.<br>
[15]RANJAN, Rajeev, et al. Deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition. U.S. Patent Application No 15/746,237, 2018.<br>
[16]REBUFFI, Sylvestre-Alvise; BILEN, Hakan; VEDALDI, Andrea. Efficient parametrization of multi-domain deep neural networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. p. 8119-8127.<br>
[17]VOIGTLAENDER, Paul, et al. MOTS: Multi-object tracking and segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2019. p. 7942-7951.<br>
[18]KENDALL, Alex; GAL, Yarin; CIPOLLA, Roberto. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. p. 7482-7491.<br>
[19]BEWLEY, Alex, et al. Simple online and realtime tracking. In: 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016. p. 3464-3468.<br>


